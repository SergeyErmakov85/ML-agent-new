{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML-Agents: –û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π API —Å DQN –∏ PPO\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ ML-–∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π API —Å Unity, –∏—Å–ø–æ–ª—å–∑—É—è exe-—Ñ–∞–π–ª –æ–±—É—á–∞—é—â–µ–π —Å—Ä–µ–¥—ã.\n",
    "\n",
    "## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n",
    "1. [–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è](#1)\n",
    "2. [–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–Ω–µ—à–Ω–µ–π Unity —Å—Ä–µ–¥–µ](#2)\n",
    "3. [–†–µ–∞–ª–∏–∑–∞—Ü–∏—è DQN –∞–ª–≥–æ—Ä–∏—Ç–º–∞](#3)\n",
    "4. [–†–µ–∞–ª–∏–∑–∞—Ü–∏—è PPO –∞–ª–≥–æ—Ä–∏—Ç–º–∞](#4)\n",
    "5. [–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤](#5)\n",
    "6. [Troubleshooting](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è <a id=\"1\"></a>\n",
    "\n",
    "–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Unity ML-Agents —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "!pip install mlagents>=1.0.0\n",
    "!pip install gym-unity>=0.29.0\n",
    "!pip install torch>=2.0.0\n",
    "!pip install torchvision>=0.15.0\n",
    "!pip install stable-baselines3>=2.0.0\n",
    "!pip install tensorboard>=2.13.0\n",
    "!pip install matplotlib>=3.7.0\n",
    "!pip install numpy>=1.24.0\n",
    "!pip install pandas>=2.0.0\n",
    "!pip install seaborn>=0.12.0\n",
    "!pip install tqdm>=4.65.0\n",
    "!pip install pillow>=9.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Unity ML-Agents imports\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel\n",
    "\n",
    "# Stable Baselines3 imports for PPO\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"–í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–Ω–µ—à–Ω–µ–π Unity —Å—Ä–µ–¥–µ <a id=\"2\"></a>\n",
    "\n",
    "–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Unity exe-—Ñ–∞–π–ª—É —á–µ—Ä–µ–∑ API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è\n",
    "class UnityConfig:\n",
    "    def __init__(self):\n",
    "        # –ü—É—Ç—å –∫ Unity exe-—Ñ–∞–π–ª—É (–∏–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π –ø—É—Ç—å)\n",
    "        self.unity_exe_path = \"./YourUnityEnvironment.exe\"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –ø—É—Ç—å\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è\n",
    "        self.base_port = 5005\n",
    "        self.no_graphics = False  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ True –¥–ª—è headless —Ä–µ–∂–∏–º–∞\n",
    "        self.time_scale = 20.0    # –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "        \n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã\n",
    "        self.max_steps = 1000000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤\n",
    "        self.num_envs = 1         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥\n",
    "        \n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "        self.episode_length = 1000\n",
    "        self.target_reward = 10.0\n",
    "\n",
    "config = UnityConfig()\n",
    "print(f\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Unity —Å—Ä–µ–¥—ã:\")\n",
    "print(f\"  - –ü—É—Ç—å –∫ exe: {config.unity_exe_path}\")\n",
    "print(f\"  - –ü–æ—Ä—Ç: {config.base_port}\")\n",
    "print(f\"  - –ë–µ–∑ –≥—Ä–∞—Ñ–∏–∫–∏: {config.no_graphics}\")\n",
    "print(f\"  - –°–∫–æ—Ä–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–∏: {config.time_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unity_environment(config):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Unity —Å—Ä–µ–¥–µ.\n",
    "    \"\"\"\n",
    "    # –°–æ–∑–¥–∞–µ–º –∫–∞–Ω–∞–ª—ã –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ä–µ–¥—ã\n",
    "    engine_config_channel = EngineConfigurationChannel()\n",
    "    env_params_channel = EnvironmentParametersChannel()\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É\n",
    "    try:\n",
    "        env = UnityEnvironment(\n",
    "            file_name=config.unity_exe_path,\n",
    "            base_port=config.base_port,\n",
    "            no_graphics=config.no_graphics,\n",
    "            side_channels=[engine_config_channel, env_params_channel]\n",
    "        )\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã\n",
    "        engine_config_channel.set_configuration_parameters(\n",
    "            time_scale=config.time_scale,\n",
    "            target_frame_rate=60,\n",
    "            capture_frame_rate=60\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Unity —Å—Ä–µ–¥–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!\")\n",
    "        return env, engine_config_channel, env_params_channel\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Unity —Å—Ä–µ–¥—ã: {e}\")\n",
    "        print(\"\\nüîß –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:\")\n",
    "        print(\"  1. –ü—É—Ç—å –∫ exe-—Ñ–∞–π–ª—É –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω\")\n",
    "        print(\"  2. Exe-—Ñ–∞–π–ª –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è\")\n",
    "        print(\"  3. –ü–æ—Ä—Ç –Ω–µ –∑–∞–Ω—è—Ç –¥—Ä—É–≥–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º\")\n",
    "        print(\"  4. Unity —Å—Ä–µ–¥–∞ —Å–æ–±—Ä–∞–Ω–∞ —Å ML-Agents\")\n",
    "        return None, None, None\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –∫–æ–≥–¥–∞ —É –≤–∞—Å –±—É–¥–µ—Ç exe-—Ñ–∞–π–ª)\n",
    "# env, engine_channel, params_channel = create_unity_environment(config)\n",
    "\n",
    "# –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–µ–º mock-—Å—Ä–µ–¥—É\n",
    "print(\"‚ö†Ô∏è  –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è mock-—Å—Ä–µ–¥–∞\")\n",
    "print(\"   –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é Unity —Å—Ä–µ–¥—É –∫–æ–≥–¥–∞ –æ–Ω–∞ –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environment_info(env):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Unity —Å—Ä–µ–¥–µ.\n",
    "    \"\"\"\n",
    "    # –°–±—Ä–æ—Å —Å—Ä–µ–¥—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
    "    env.reset()\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ behavior\n",
    "    behavior_names = list(env.behavior_specs.keys())\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ {len(behavior_names)} behavior(s): {behavior_names}\")\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π behavior\n",
    "    for behavior_name in behavior_names:\n",
    "        spec = env.behavior_specs[behavior_name]\n",
    "        print(f\"\\nüìä Behavior: {behavior_name}\")\n",
    "        print(f\"  - –†–∞–∑–º–µ—Ä –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {spec.observation_specs[0].shape}\")\n",
    "        print(f\"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π: {spec.action_spec.continuous_size + spec.action_spec.discrete_size}\")\n",
    "        print(f\"  - –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è: {spec.action_spec.continuous_size}\")\n",
    "        print(f\"  - –î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è: {spec.action_spec.discrete_size}\")\n",
    "        \n",
    "        if spec.action_spec.discrete_size > 0:\n",
    "            print(f\"  - –í–µ—Ç–≤–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π: {spec.action_spec.discrete_branches}\")\n",
    "    \n",
    "    return behavior_names\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ä–µ–¥–µ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥—ã)\n",
    "# behavior_names = get_environment_info(env)\n",
    "\n",
    "# –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–µ–º mock-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "class MockEnvironmentInfo:\n",
    "    def __init__(self):\n",
    "        self.behavior_names = [\"MyAgent\"]\n",
    "        self.observation_size = 8\n",
    "        self.action_size = 4\n",
    "        self.continuous_actions = 2\n",
    "        self.discrete_actions = 2\n",
    "\n",
    "mock_env_info = MockEnvironmentInfo()\n",
    "print(\"üéØ Mock-—Å—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:\")\n",
    "print(f\"  - –†–∞–∑–º–µ—Ä –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {mock_env_info.observation_size}\")\n",
    "print(f\"  - –†–∞–∑–º–µ—Ä –¥–µ–π—Å—Ç–≤–∏–π: {mock_env_info.action_size}\")\n",
    "print(f\"  - –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è: {mock_env_info.continuous_actions}\")\n",
    "print(f\"  - –î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è: {mock_env_info.discrete_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è DQN –∞–ª–≥–æ—Ä–∏—Ç–º–∞ <a id=\"3\"></a>\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–µ–º Deep Q-Network —Å –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–∞—è Q-—Å–µ—Ç—å –¥–ª—è DQN.\n",
    "    –í—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –¥–æ–±–∞–≤–∏—Ç—å —Å–ª–æ–∏, –∏–∑–º–µ–Ω–∏—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_layers=None, activation='relu', dropout=0.0):\n",
    "        super(ConfigurableQNetwork, self).__init__()\n",
    "        \n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "        if hidden_layers is None:\n",
    "            hidden_layers = [256, 256, 128]  # –ú–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∑–¥–µ—Å—å\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–µ–≤\n",
    "        layers = []\n",
    "        input_size = state_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(self.activation)\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
    "        layers.append(nn.Linear(input_size, action_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ Xavier/He.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è ReLU\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥.\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π.\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.forward(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä\n",
    "print(\"üß† –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä Q-—Å–µ—Ç–∏:\")\n",
    "print(\"\\n1. –ü—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\")\n",
    "simple_net = ConfigurableQNetwork(8, 4, hidden_layers=[64, 64])\n",
    "print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in simple_net.parameters())}\")\n",
    "\n",
    "print(\"\\n2. –ì–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\")\n",
    "deep_net = ConfigurableQNetwork(8, 4, hidden_layers=[512, 256, 128, 64])\n",
    "print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in deep_net.parameters())}\")\n",
    "\n",
    "print(\"\\n3. –®–∏—Ä–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\")\n",
    "wide_net = ConfigurableQNetwork(8, 4, hidden_layers=[1024, 512])\n",
    "print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in wide_net.parameters())}\")\n",
    "\n",
    "print(\"\\n4. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å dropout:\")\n",
    "dropout_net = ConfigurableQNetwork(8, 4, hidden_layers=[256, 256], dropout=0.3)\n",
    "print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in dropout_net.parameters())}\")\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    –ë—É—Ñ–µ—Ä –æ–ø—ã—Ç–∞ –¥–ª—è DQN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple('Experience', \n",
    "                                   ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"–î–æ–±–∞–≤–ª—è–µ—Ç –æ–ø—ã—Ç –≤ –±—É—Ñ–µ—Ä.\"\"\"\n",
    "        exp = self.experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(exp)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"–í—ã–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –ø–∞—Ä—Ç–∏—é –∏–∑ –±—É—Ñ–µ—Ä–∞.\"\"\"\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor([e.state for e in experiences]).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in experiences]).to(device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)\n",
    "        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device)\n",
    "        dones = torch.BoolTensor([e.done for e in experiences]).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±—É—Ñ–µ—Ä–∞\n",
    "buffer = ReplayBuffer(10000)\n",
    "print(f\"üíæ –ë—É—Ñ–µ—Ä –æ–ø—ã—Ç–∞ —Å–æ–∑–¥–∞–Ω, –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {buffer.buffer.maxlen}\")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –æ–ø—ã—Ç\n",
    "for i in range(5):\n",
    "    state = np.random.random(8)\n",
    "    action = np.random.randint(0, 4)\n",
    "    reward = np.random.random()\n",
    "    next_state = np.random.random(8)\n",
    "    done = np.random.choice([True, False])\n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(buffer)} –æ–ø—ã—Ç–æ–≤ –≤ –±—É—Ñ–µ—Ä\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    –ê–≥–µ–Ω—Ç DQN —Å –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, config=None):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "        if config is None:\n",
    "            config = {\n",
    "                'learning_rate': 0.001,\n",
    "                'gamma': 0.99,\n",
    "                'epsilon_start': 1.0,\n",
    "                'epsilon_end': 0.01,\n",
    "                'epsilon_decay': 0.995,\n",
    "                'batch_size': 64,\n",
    "                'buffer_size': 100000,\n",
    "                'target_update_freq': 1000,\n",
    "                'hidden_layers': [256, 256],\n",
    "                'activation': 'relu',\n",
    "                'dropout': 0.0,\n",
    "                'optimizer': 'adam',\n",
    "                'loss_function': 'huber'  # –∏–ª–∏ 'mse'\n",
    "            }\n",
    "        \n",
    "        self.config = config\n",
    "        self.epsilon = config['epsilon_start']\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∏\n",
    "        self.q_network = ConfigurableQNetwork(\n",
    "            state_size, action_size, \n",
    "            hidden_layers=config['hidden_layers'],\n",
    "            activation=config['activation'],\n",
    "            dropout=config['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        self.target_network = ConfigurableQNetwork(\n",
    "            state_size, action_size,\n",
    "            hidden_layers=config['hidden_layers'],\n",
    "            activation=config['activation'],\n",
    "            dropout=config['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º target network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "        if config['optimizer'] == 'adam':\n",
    "            self.optimizer = optim.Adam(self.q_network.parameters(), lr=config['learning_rate'])\n",
    "        elif config['optimizer'] == 'rmsprop':\n",
    "            self.optimizer = optim.RMSprop(self.q_network.parameters(), lr=config['learning_rate'])\n",
    "        elif config['optimizer'] == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.q_network.parameters(), lr=config['learning_rate'])\n",
    "        \n",
    "        # –ë—É—Ñ–µ—Ä –æ–ø—ã—Ç–∞\n",
    "        self.replay_buffer = ReplayBuffer(config['buffer_size'])\n",
    "        \n",
    "        # –°—á–µ—Ç—á–∏–∫–∏\n",
    "        self.step_count = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "        self.epsilons = []\n",
    "        \n",
    "        print(f\"ü§ñ DQN –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω:\")\n",
    "        print(f\"  - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {config['hidden_layers']}\")\n",
    "        print(f\"  - –ê–∫—Ç–∏–≤–∞—Ü–∏—è: {config['activation']}\")\n",
    "        print(f\"  - –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {config['optimizer']}\")\n",
    "        print(f\"  - Learning rate: {config['learning_rate']}\")\n",
    "        print(f\"  - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏: {sum(p.numel() for p in self.q_network.parameters())}\")\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π.\"\"\"\n",
    "        return self.q_network.get_action(state, self.epsilon)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"–û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è.\"\"\"\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        # –û–±—É—á–µ–Ω–∏–µ\n",
    "        if len(self.replay_buffer) >= self.config['batch_size']:\n",
    "            self.learn()\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ target network\n",
    "        if self.step_count % self.config['target_update_freq'] == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä—Ç–∏–∏ –∏–∑ –±—É—Ñ–µ—Ä–∞.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.config['batch_size'])\n",
    "        \n",
    "        # –¢–µ–∫—É—â–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # –°–ª–µ–¥—É—é—â–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.config['gamma'] * next_q_values * ~dones)\n",
    "        \n",
    "        # –ü–æ—Ç–µ—Ä—è\n",
    "        if self.config['loss_function'] == 'huber':\n",
    "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "        else:\n",
    "            loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ epsilon\n",
    "        self.epsilon = max(self.config['epsilon_end'], \n",
    "                          self.epsilon * self.config['epsilon_decay'])\n",
    "        self.epsilons.append(self.epsilon)\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.\"\"\"\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "            'step_count': self.step_count,\n",
    "            'episode_count': self.episode_count,\n",
    "            'epsilon': self.epsilon\n",
    "        }, filepath)\n",
    "        print(f\"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.step_count = checkpoint['step_count']\n",
    "        self.episode_count = checkpoint['episode_count']\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        print(f\"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DQN –∞–≥–µ–Ω—Ç–∞\n",
    "dqn_config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'batch_size': 64,\n",
    "    'buffer_size': 100000,\n",
    "    'target_update_freq': 1000,\n",
    "    'hidden_layers': [256, 256, 128],  # –ò–∑–º–µ–Ω–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∑–¥–µ—Å—å\n",
    "    'activation': 'relu',  # relu, tanh, leaky_relu, elu\n",
    "    'dropout': 0.0,\n",
    "    'optimizer': 'adam',  # adam, rmsprop, sgd\n",
    "    'loss_function': 'huber'  # huber, mse\n",
    "}\n",
    "\n",
    "dqn_agent = DQNAgent(mock_env_info.observation_size, mock_env_info.action_size, dqn_config)\n",
    "print(\"\\n‚úÖ DQN –∞–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ DQN –∞–≥–µ–Ω—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_agent(agent, env=None, episodes=1000, max_steps=1000, save_freq=100):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–µ–Ω–∏–µ DQN –∞–≥–µ–Ω—Ç–∞.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ DQN –Ω–∞ {episodes} —ç–ø–∏–∑–æ–¥–æ–≤...\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä\n",
    "    pbar = tqdm(range(episodes), desc=\"DQN Training\")\n",
    "    \n",
    "    for episode in pbar:\n",
    "        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º mock-—Å—Ä–µ–¥—É\n",
    "        if env is None:\n",
    "            # Mock environment –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "            state = np.random.random(agent.state_size)\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ\n",
    "                action = agent.get_action(state)\n",
    "                \n",
    "                # Mock –ø–µ—Ä–µ—Ö–æ–¥\n",
    "                next_state = np.random.random(agent.state_size)\n",
    "                reward = np.random.random() - 0.5  # –°–ª—É—á–∞–π–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞\n",
    "                done = np.random.random() < 0.01  # 1% —à–∞–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è\n",
    "                \n",
    "                # –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "        else:\n",
    "            # –†–µ–∞–ª—å–Ω–∞—è Unity —Å—Ä–µ–¥–∞\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(agent.behavior_name)\n",
    "            \n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while len(decision_steps) > 0 and steps < max_steps:\n",
    "                # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è\n",
    "                observations = decision_steps.obs[0]\n",
    "                \n",
    "                # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤\n",
    "                actions = []\n",
    "                for obs in observations:\n",
    "                    action = agent.get_action(obs)\n",
    "                    actions.append(action)\n",
    "                \n",
    "                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–µ–π—Å—Ç–≤–∏—è –≤ Unity\n",
    "                action_tuple = ActionTuple(discrete=np.array([actions]))\n",
    "                env.set_actions(agent.behavior_name, action_tuple)\n",
    "                env.step()\n",
    "                \n",
    "                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "                decision_steps, terminal_steps = env.get_steps(agent.behavior_name)\n",
    "                \n",
    "                # –û–±—Ä–∞–±–æ—Ç–∫–∞ terminal steps\n",
    "                for i, agent_id in enumerate(terminal_steps.agent_id):\n",
    "                    reward = terminal_steps.reward[i]\n",
    "                    episode_reward += reward\n",
    "                    # –î–æ–±–∞–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –∑–¥–µ—Å—å\n",
    "                \n",
    "                steps += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        agent.episode_count += 1\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            pbar.set_postfix({\n",
    "                'Avg Reward': f'{avg_reward:.2f}',\n",
    "                'Epsilon': f'{agent.epsilon:.3f}',\n",
    "                'Buffer': len(agent.replay_buffer)\n",
    "            })\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        if episode % save_freq == 0 and episode > 0:\n",
    "            agent.save_model(f'dqn_model_episode_{episode}.pth')\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ DQN (–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ mock-—Å—Ä–µ–¥–µ)\n",
    "print(\"‚ö†Ô∏è  –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ mock-—Å—Ä–µ–¥–µ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é Unity —Å—Ä–µ–¥—É)\")\n",
    "dqn_rewards, dqn_lengths = train_dqn_agent(dqn_agent, episodes=100)\n",
    "\n",
    "print(f\"\\n‚úÖ –û–±—É—á–µ–Ω–∏–µ DQN –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "print(f\"  - –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(dqn_rewards):.2f}\")\n",
    "print(f\"  - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {np.mean(dqn_lengths):.2f}\")\n",
    "print(f\"  - –§–∏–Ω–∞–ª—å–Ω—ã–π epsilon: {dqn_agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è PPO –∞–ª–≥–æ—Ä–∏—Ç–º–∞ <a id=\"4\"></a>\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–µ–º Proximal Policy Optimization —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurablePPONetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–∞—è —Å–µ—Ç—å –¥–ª—è PPO —Å policy –∏ value –≥–æ–ª–æ–≤–∞–º–∏.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_layers=None, activation='relu', \n",
    "                 dropout=0.0, separate_networks=False):\n",
    "        super(ConfigurablePPONetwork, self).__init__()\n",
    "        \n",
    "        if hidden_layers is None:\n",
    "            hidden_layers = [256, 256]\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.separate_networks = separate_networks\n",
    "        \n",
    "        # –í—ã–±–æ—Ä –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        if separate_networks:\n",
    "            # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è policy –∏ value\n",
    "            self.policy_net = self._build_network(state_size, hidden_layers, dropout)\n",
    "            self.value_net = self._build_network(state_size, hidden_layers, dropout)\n",
    "            \n",
    "            self.policy_head = nn.Linear(hidden_layers[-1], action_size)\n",
    "            self.value_head = nn.Linear(hidden_layers[-1], 1)\n",
    "        else:\n",
    "            # –û–±—â–∞—è —Å–µ—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ –≥–æ–ª–æ–≤–∞–º–∏\n",
    "            self.shared_net = self._build_network(state_size, hidden_layers, dropout)\n",
    "            \n",
    "            self.policy_head = nn.Linear(hidden_layers[-1], action_size)\n",
    "            self.value_head = nn.Linear(hidden_layers[-1], 1)\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _build_network(self, input_size, hidden_layers, dropout):\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ç–∏.\"\"\"\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(self.activation)\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, np.sqrt(2))\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥.\"\"\"\n",
    "        if self.separate_networks:\n",
    "            policy_features = self.policy_net(state)\n",
    "            value_features = self.value_net(state)\n",
    "            \n",
    "            policy_logits = self.policy_head(policy_features)\n",
    "            value = self.value_head(value_features)\n",
    "        else:\n",
    "            shared_features = self.shared_net(state)\n",
    "            \n",
    "            policy_logits = self.policy_head(shared_features)\n",
    "            value = self.value_head(shared_features)\n",
    "        \n",
    "        return policy_logits, value\n",
    "    \n",
    "    def get_action_and_value(self, state):\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –∏ –∑–Ω–∞—á–µ–Ω–∏—è.\"\"\"\n",
    "        policy_logits, value = self.forward(state)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "        dist = torch.distributions.Categorical(logits=policy_logits)\n",
    "        \n",
    "        # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ\n",
    "        action = dist.sample()\n",
    "        \n",
    "        return action, dist.log_prob(action), value, dist.entropy()\n",
    "    \n",
    "    def evaluate_actions(self, state, actions):\n",
    "        \"\"\"–û—Ü–µ–Ω–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\"\"\"\n",
    "        policy_logits, value = self.forward(state)\n",
    "        \n",
    "        dist = torch.distributions.Categorical(logits=policy_logits)\n",
    "        \n",
    "        action_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return action_log_probs, value, entropy\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö PPO –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä\n",
    "print(\"üß† –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä PPO-—Å–µ—Ç–∏:\")\n",
    "\n",
    "print(\"\\n1. –û–±—â–∞—è —Å–µ—Ç—å:\")\n",
    "shared_net = ConfigurablePPONetwork(8, 4, hidden_layers=[256, 256], separate_networks=False)\n",
    "print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in shared_net.parameters())}\")\n",
    "\n",
    "print(\"\\n2. –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å–µ—Ç–∏:\")\n",
    "separate_net = ConfigurablePPONetwork(8, 4, hidden_layers=[256, 256], separate_networks=True)\n",
    "print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in separate_net.parameters())}\")\n",
    "\n",
    "print(\"\\n3. –ì–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\")\n",
    "deep_ppo_net = ConfigurablePPONetwork(8, 4, hidden_layers=[512, 256, 128], dropout=0.2)\n",
    "print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in deep_ppo_net.parameters())}\")\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ PPO –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    PPO –∞–≥–µ–Ω—Ç —Å –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, config=None):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "        if config is None:\n",
    "            config = {\n",
    "                'learning_rate': 3e-4,\n",
    "                'gamma': 0.99,\n",
    "                'gae_lambda': 0.95,\n",
    "                'clip_param': 0.2,\n",
    "                'value_loss_coef': 0.5,\n",
    "                'entropy_coef': 0.01,\n",
    "                'max_grad_norm': 0.5,\n",
    "                'ppo_epochs': 4,\n",
    "                'batch_size': 64,\n",
    "                'n_steps': 2048,\n",
    "                'hidden_layers': [256, 256],\n",
    "                'activation': 'tanh',\n",
    "                'dropout': 0.0,\n",
    "                'separate_networks': False,\n",
    "                'optimizer': 'adam',\n",
    "                'lr_schedule': 'constant'  # constant, linear, cosine\n",
    "            }\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç—å\n",
    "        self.network = ConfigurablePPONetwork(\n",
    "            state_size, action_size,\n",
    "            hidden_layers=config['hidden_layers'],\n",
    "            activation=config['activation'],\n",
    "            dropout=config['dropout'],\n",
    "            separate_networks=config['separate_networks']\n",
    "        ).to(device)\n",
    "        \n",
    "        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "        if config['optimizer'] == 'adam':\n",
    "            self.optimizer = optim.Adam(self.network.parameters(), lr=config['learning_rate'])\n",
    "        elif config['optimizer'] == 'rmsprop':\n",
    "            self.optimizer = optim.RMSprop(self.network.parameters(), lr=config['learning_rate'])\n",
    "        elif config['optimizer'] == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.network.parameters(), lr=config['learning_rate'])\n",
    "        \n",
    "        # –ë—É—Ñ–µ—Ä—ã –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        self.episode_rewards = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropy_losses = []\n",
    "        \n",
    "        # –°—á–µ—Ç—á–∏–∫–∏\n",
    "        self.step_count = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        print(f\"ü§ñ PPO –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω:\")\n",
    "        print(f\"  - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {config['hidden_layers']}\")\n",
    "        print(f\"  - –ê–∫—Ç–∏–≤–∞—Ü–∏—è: {config['activation']}\")\n",
    "        print(f\"  - –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å–µ—Ç–∏: {config['separate_networks']}\")\n",
    "        print(f\"  - –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {config['optimizer']}\")\n",
    "        print(f\"  - Learning rate: {config['learning_rate']}\")\n",
    "        print(f\"  - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏: {sum(p.numel() for p in self.network.parameters())}\")\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action, log_prob, value, entropy = self.network.get_action_and_value(state_tensor)\n",
    "            \n",
    "            return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, done, value, log_prob):\n",
    "        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥–∞.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "    \n",
    "    def compute_gae(self, next_value=0):\n",
    "        \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Generalized Advantage Estimation.\"\"\"\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        values = self.values + [next_value]\n",
    "        \n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            delta = self.rewards[step] + self.config['gamma'] * values[step + 1] * (1 - self.dones[step]) - values[step]\n",
    "            gae = delta + self.config['gamma'] * self.config['gae_lambda'] * (1 - self.dones[step]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        returns = [adv + val for adv, val in zip(advantages, self.values)]\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def update(self, next_value=0):\n",
    "        \"\"\"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ—Ç–∏ PPO.\"\"\"\n",
    "        if len(self.states) < self.config['n_steps']:\n",
    "            return\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º GAE\n",
    "        advantages, returns = self.compute_gae(next_value)\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "        states = torch.FloatTensor(self.states).to(device)\n",
    "        actions = torch.LongTensor(self.actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.log_probs).to(device)\n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n",
    "        for epoch in range(self.config['ppo_epochs']):\n",
    "            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            # –ú–∏–Ω–∏-–±–∞—Ç—á–∏\n",
    "            for start in range(0, len(states), self.config['batch_size']):\n",
    "                end = start + self.config['batch_size']\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ log_probs –∏ values\n",
    "                new_log_probs, new_values, entropy = self.network.evaluate_actions(batch_states, batch_actions)\n",
    "                \n",
    "                # Ratio –¥–ª—è PPO\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Surrogate loss\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(new_values.squeeze(), batch_returns)\n",
    "                \n",
    "                # Entropy loss\n",
    "                entropy_loss = -entropy.mean()\n",
    "                \n",
    "                # –û–±—â–∞—è loss\n",
    "                total_loss = (policy_loss + \n",
    "                             self.config['value_loss_coef'] * value_loss + \n",
    "                             self.config['entropy_coef'] * entropy_loss)\n",
    "                \n",
    "                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.config['max_grad_norm'])\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "                self.policy_losses.append(policy_loss.item())\n",
    "                self.value_losses.append(value_loss.item())\n",
    "                self.entropy_losses.append(entropy_loss.item())\n",
    "        \n",
    "        # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä—ã\n",
    "        self.clear_buffers()\n",
    "    \n",
    "    def clear_buffers(self):\n",
    "        \"\"\"–û—á–∏—Å—Ç–∫–∞ –±—É—Ñ–µ—Ä–æ–≤.\"\"\"\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.dones.clear()\n",
    "        self.values.clear()\n",
    "        self.log_probs.clear()\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.\"\"\"\n",
    "        torch.save({\n",
    "            'network_state_dict': self.network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "            'step_count': self.step_count,\n",
    "            'episode_count': self.episode_count\n",
    "        }, filepath)\n",
    "        print(f\"üíæ PPO –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        self.network.load_state_dict(checkpoint['network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.step_count = checkpoint['step_count']\n",
    "        self.episode_count = checkpoint['episode_count']\n",
    "        print(f\"üìÇ PPO –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º PPO –∞–≥–µ–Ω—Ç–∞\n",
    "ppo_config = {\n",
    "    'learning_rate': 3e-4,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_param': 0.2,\n",
    "    'value_loss_coef': 0.5,\n",
    "    'entropy_coef': 0.01,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'ppo_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'n_steps': 2048,\n",
    "    'hidden_layers': [256, 256],  # –ò–∑–º–µ–Ω–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∑–¥–µ—Å—å\n",
    "    'activation': 'tanh',  # relu, tanh, leaky_relu, elu\n",
    "    'dropout': 0.0,\n",
    "    'separate_networks': False,  # True –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
    "    'optimizer': 'adam',  # adam, rmsprop, sgd\n",
    "    'lr_schedule': 'constant'\n",
    "}\n",
    "\n",
    "ppo_agent = PPOAgent(mock_env_info.observation_size, mock_env_info.action_size, ppo_config)\n",
    "print(\"\\n‚úÖ PPO –∞–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ PPO –∞–≥–µ–Ω—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo_agent(agent, env=None, episodes=1000, max_steps=1000, update_freq=10):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–µ–Ω–∏–µ PPO –∞–≥–µ–Ω—Ç–∞.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ PPO –Ω–∞ {episodes} —ç–ø–∏–∑–æ–¥–æ–≤...\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä\n",
    "    pbar = tqdm(range(episodes), desc=\"PPO Training\")\n",
    "    \n",
    "    for episode in pbar:\n",
    "        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º mock-—Å—Ä–µ–¥—É\n",
    "        if env is None:\n",
    "            # Mock environment –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "            state = np.random.random(agent.state_size)\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ\n",
    "                action, log_prob, value = agent.get_action(state)\n",
    "                \n",
    "                # Mock –ø–µ—Ä–µ—Ö–æ–¥\n",
    "                next_state = np.random.random(agent.state_size)\n",
    "                reward = np.random.random() - 0.5  # –°–ª—É—á–∞–π–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞\n",
    "                done = np.random.random() < 0.01  # 1% —à–∞–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è\n",
    "                \n",
    "                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥\n",
    "                agent.store_transition(state, action, reward, done, value, log_prob)\n",
    "                \n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                agent.step_count += 1\n",
    "                \n",
    "                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ n_steps\n",
    "                if len(agent.states) >= agent.config['n_steps']:\n",
    "                    next_value = agent.get_action(state)[2] if not done else 0\n",
    "                    agent.update(next_value)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "        else:\n",
    "            # –†–µ–∞–ª—å–Ω–∞—è Unity —Å—Ä–µ–¥–∞\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(agent.behavior_name)\n",
    "            \n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while len(decision_steps) > 0 and steps < max_steps:\n",
    "                # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è\n",
    "                observations = decision_steps.obs[0]\n",
    "                \n",
    "                # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤\n",
    "                actions = []\n",
    "                log_probs = []\n",
    "                values = []\n",
    "                \n",
    "                for obs in observations:\n",
    "                    action, log_prob, value = agent.get_action(obs)\n",
    "                    actions.append(action)\n",
    "                    log_probs.append(log_prob)\n",
    "                    values.append(value)\n",
    "                \n",
    "                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–µ–π—Å—Ç–≤–∏—è –≤ Unity\n",
    "                action_tuple = ActionTuple(discrete=np.array([actions]))\n",
    "                env.set_actions(agent.behavior_name, action_tuple)\n",
    "                env.step()\n",
    "                \n",
    "                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "                decision_steps, terminal_steps = env.get_steps(agent.behavior_name)\n",
    "                \n",
    "                # –û–±—Ä–∞–±–æ—Ç–∫–∞ terminal steps\n",
    "                for i, agent_id in enumerate(terminal_steps.agent_id):\n",
    "                    reward = terminal_steps.reward[i]\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥\n",
    "                    agent.store_transition(\n",
    "                        observations[i], actions[i], reward, True, \n",
    "                        values[i], log_probs[i]\n",
    "                    )\n",
    "                \n",
    "                steps += 1\n",
    "                agent.step_count += 1\n",
    "                \n",
    "                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ n_steps\n",
    "                if len(agent.states) >= agent.config['n_steps']:\n",
    "                    agent.update()\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        agent.episode_count += 1\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            pbar.set_postfix({\n",
    "                'Avg Reward': f'{avg_reward:.2f}',\n",
    "                'Steps': agent.step_count,\n",
    "                'Buffer': len(agent.states)\n",
    "            })\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        if episode % 100 == 0 and episode > 0:\n",
    "            agent.save_model(f'ppo_model_episode_{episode}.pth')\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ PPO (–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ mock-—Å—Ä–µ–¥–µ)\n",
    "print(\"‚ö†Ô∏è  –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ mock-—Å—Ä–µ–¥–µ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é Unity —Å—Ä–µ–¥—É)\")\n",
    "ppo_rewards, ppo_lengths = train_ppo_agent(ppo_agent, episodes=100)\n",
    "\n",
    "print(f\"\\n‚úÖ –û–±—É—á–µ–Ω–∏–µ PPO –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "print(f\"  - –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(ppo_rewards):.2f}\")\n",
    "print(f\"  - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {np.mean(ppo_lengths):.2f}\")\n",
    "print(f\"  - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤: {ppo_agent.step_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ <a id=\"5\"></a>\n",
    "\n",
    "–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è DQN –∏ PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(dqn_rewards, ppo_rewards, dqn_lengths, ppo_lengths):\n",
    "    \"\"\"\n",
    "    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è\n",
    "    def moving_average(data, window=10):\n",
    "        return pd.Series(data).rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 1: –ù–∞–≥—Ä–∞–¥—ã –ø–æ —ç–ø–∏–∑–æ–¥–∞–º\n",
    "    axes[0, 0].plot(moving_average(dqn_rewards), label='DQN', color='blue', alpha=0.7)\n",
    "    axes[0, 0].plot(moving_average(ppo_rewards), label='PPO', color='red', alpha=0.7)\n",
    "    axes[0, 0].set_title('–ù–∞–≥—Ä–∞–¥—ã –ø–æ —ç–ø–∏–∑–æ–¥–∞–º (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ)')\n",
    "    axes[0, 0].set_xlabel('–≠–ø–∏–∑–æ–¥')\n",
    "    axes[0, 0].set_ylabel('–ù–∞–≥—Ä–∞–¥–∞')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 2: –î–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–æ–≤\n",
    "    axes[0, 1].plot(moving_average(dqn_lengths), label='DQN', color='blue', alpha=0.7)\n",
    "    axes[0, 1].plot(moving_average(ppo_lengths), label='PPO', color='red', alpha=0.7)\n",
    "    axes[0, 1].set_title('–î–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–æ–≤ (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ)')\n",
    "    axes[0, 1].set_xlabel('–≠–ø–∏–∑–æ–¥')\n",
    "    axes[0, 1].set_ylabel('–®–∞–≥–∏')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥\n",
    "    axes[1, 0].hist(dqn_rewards, bins=30, alpha=0.7, label='DQN', color='blue')\n",
    "    axes[1, 0].hist(ppo_rewards, bins=30, alpha=0.7, label='PPO', color='red')\n",
    "    axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥')\n",
    "    axes[1, 0].set_xlabel('–ù–∞–≥—Ä–∞–¥–∞')\n",
    "    axes[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 4: –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã\n",
    "    cumulative_dqn = np.cumsum(dqn_rewards)\n",
    "    cumulative_ppo = np.cumsum(ppo_rewards)\n",
    "    axes[1, 1].plot(cumulative_dqn, label='DQN', color='blue', alpha=0.7)\n",
    "    axes[1, 1].plot(cumulative_ppo, label='PPO', color='red', alpha=0.7)\n",
    "    axes[1, 1].set_title('–ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã')\n",
    "    axes[1, 1].set_xlabel('–≠–ø–∏–∑–æ–¥')\n",
    "    axes[1, 1].set_ylabel('–ù–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "    print(\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "    print(f\"\\nDQN:\")\n",
    "    print(f\"  - –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(dqn_rewards):.3f} ¬± {np.std(dqn_rewards):.3f}\")\n",
    "    print(f\"  - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {np.max(dqn_rewards):.3f}\")\n",
    "    print(f\"  - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {np.min(dqn_rewards):.3f}\")\n",
    "    print(f\"  - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {np.mean(dqn_lengths):.1f}\")\n",
    "    \n",
    "    print(f\"\\nPPO:\")\n",
    "    print(f\"  - –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(ppo_rewards):.3f} ¬± {np.std(ppo_rewards):.3f}\")\n",
    "    print(f\"  - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {np.max(ppo_rewards):.3f}\")\n",
    "    print(f\"  - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {np.min(ppo_rewards):.3f}\")\n",
    "    print(f\"  - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {np.mean(ppo_lengths):.1f}\")\n",
    "\n",
    "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "plot_training_results(dqn_rewards, ppo_rewards, dqn_lengths, ppo_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(dqn_agent, ppo_agent):\n",
    "    \"\"\"\n",
    "    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤—ã—Ö –ø–æ—Ç–µ—Ä—å –¥–ª—è –æ–±–æ–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # DQN –ø–æ—Ç–µ—Ä–∏\n",
    "    if dqn_agent.losses:\n",
    "        axes[0].plot(dqn_agent.losses, color='blue', alpha=0.7)\n",
    "        axes[0].set_title('DQN: Q-Learning Loss')\n",
    "        axes[0].set_xlabel('–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PPO –ø–æ—Ç–µ—Ä–∏\n",
    "    if ppo_agent.policy_losses:\n",
    "        axes[1].plot(ppo_agent.policy_losses, label='Policy Loss', color='red', alpha=0.7)\n",
    "        axes[1].plot(ppo_agent.value_losses, label='Value Loss', color='green', alpha=0.7)\n",
    "        axes[1].plot(ppo_agent.entropy_losses, label='Entropy Loss', color='orange', alpha=0.7)\n",
    "        axes[1].set_title('PPO: –†–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏')\n",
    "        axes[1].set_xlabel('–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Epsilon decay –¥–ª—è DQN\n",
    "    if dqn_agent.epsilons:\n",
    "        axes[2].plot(dqn_agent.epsilons, color='purple', alpha=0.7)\n",
    "        axes[2].set_title('DQN: Epsilon Decay')\n",
    "        axes[2].set_xlabel('–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ')\n",
    "        axes[2].set_ylabel('Epsilon')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤—ã—Ö –ø–æ—Ç–µ—Ä—å\n",
    "plot_loss_curves(dqn_agent, ppo_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_report(dqn_agent, ppo_agent, dqn_rewards, ppo_rewards):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'DQN': {\n",
    "            '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞': dqn_agent.config['hidden_layers'],\n",
    "            '–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏': sum(p.numel() for p in dqn_agent.q_network.parameters()),\n",
    "            '–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞': np.mean(dqn_rewards),\n",
    "            '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ': np.std(dqn_rewards),\n",
    "            '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞': np.max(dqn_rewards),\n",
    "            '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞': np.min(dqn_rewards),\n",
    "            '–§–∏–Ω–∞–ª—å–Ω—ã–π epsilon': dqn_agent.epsilon,\n",
    "            '–†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞': len(dqn_agent.replay_buffer),\n",
    "            '–≠–ø–∏–∑–æ–¥—ã': dqn_agent.episode_count,\n",
    "            '–®–∞–≥–∏': dqn_agent.step_count\n",
    "        },\n",
    "        'PPO': {\n",
    "            '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞': ppo_agent.config['hidden_layers'],\n",
    "            '–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏': sum(p.numel() for p in ppo_agent.network.parameters()),\n",
    "            '–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞': np.mean(ppo_rewards),\n",
    "            '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ': np.std(ppo_rewards),\n",
    "            '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞': np.max(ppo_rewards),\n",
    "            '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞': np.min(ppo_rewards),\n",
    "            'Clip –ø–∞—Ä–∞–º–µ—Ç—Ä': ppo_agent.config['clip_param'],\n",
    "            'Learning rate': ppo_agent.config['learning_rate'],\n",
    "            '–≠–ø–∏–∑–æ–¥—ã': ppo_agent.episode_count,\n",
    "            '–®–∞–≥–∏': ppo_agent.step_count\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    df = pd.DataFrame(report).T\n",
    "    \n",
    "    print(\"üìã –û—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for metric in df.columns:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        for algorithm in df.index:\n",
    "            value = df.loc[algorithm, metric]\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {algorithm}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {algorithm}: {value}\")\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç\n",
    "    with open('performance_report.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"–û—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Unity ML-Agents\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        for metric in df.columns:\n",
    "            f.write(f\"{metric}:\\n\")\n",
    "            for algorithm in df.index:\n",
    "                value = df.loc[algorithm, metric]\n",
    "                if isinstance(value, float):\n",
    "                    f.write(f\"  {algorithm}: {value:.4f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"  {algorithm}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(\"\\nüíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'performance_report.txt'\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç\n",
    "performance_report = create_performance_report(dqn_agent, ppo_agent, dqn_rewards, ppo_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Troubleshooting <a id=\"6\"></a>\n",
    "\n",
    "–†–µ—à–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å Unity ML-Agents —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ Unity\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ 1: UnityTimeoutException**\n",
    "```python\n",
    "# –†–µ—à–µ–Ω–∏–µ: –£–≤–µ–ª–∏—á—å—Ç–µ —Ç–∞–π–º–∞—É—Ç –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Ä—Ç—ã\n",
    "env = UnityEnvironment(\n",
    "    file_name=\"path/to/your/environment.exe\",\n",
    "    base_port=5005,\n",
    "    timeout_wait=60  # –£–≤–µ–ª–∏—á—å—Ç–µ —Ç–∞–π–º–∞—É—Ç\n",
    ")\n",
    "```\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ 2: –ü–æ—Ä—Ç —É–∂–µ –∑–∞–Ω—è—Ç**\n",
    "```python\n",
    "# –†–µ—à–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥—Ä—É–≥–æ–π –ø–æ—Ä—Ç –∏–ª–∏ –Ω–∞–π–¥–∏—Ç–µ —Å–≤–æ–±–æ–¥–Ω—ã–π\n",
    "import socket\n",
    "\n",
    "def find_free_port(start_port=5005):\n",
    "    for port in range(start_port, start_port + 100):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "free_port = find_free_port()\n",
    "print(f\"–°–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç: {free_port}\")\n",
    "```\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ 3: Exe-—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω**\n",
    "```python\n",
    "# –†–µ—à–µ–Ω–∏–µ: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
    "import os\n",
    "\n",
    "def check_unity_exe(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}\")\n",
    "        return False\n",
    "    \n",
    "    if not path.endswith('.exe'):\n",
    "        print(f\"‚ùå –ù–µ exe-—Ñ–∞–π–ª: {path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω: {path}\")\n",
    "    return True\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞\n",
    "exe_path = \"./YourUnityEnvironment.exe\"\n",
    "check_unity_exe(exe_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–±–ª–µ–º—ã —Å –æ–±—É—á–µ–Ω–∏–µ–º\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**\n",
    "```python\n",
    "# –†–µ—à–µ–Ω–∏–µ: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "optimized_dqn_config = {\n",
    "    'learning_rate': 0.0005,  # –£–º–µ–Ω—å—à–∏—Ç–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "    'batch_size': 128,        # –£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "    'target_update_freq': 500, # –£–º–µ–Ω—å—à–∏—Ç–µ –¥–ª—è —á–∞—Å—Ç—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π\n",
    "    'epsilon_decay': 0.999,   # –ú–µ–¥–ª–µ–Ω–Ω–µ–µ —É–º–µ–Ω—å—à–∞–π—Ç–µ exploration\n",
    "    'gamma': 0.995,          # –ë–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –±—É–¥—É—â–∏–º –Ω–∞–≥—Ä–∞–¥–∞–º\n",
    "    'hidden_layers': [512, 256, 128]  # –£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä —Å–µ—Ç–∏\n",
    "}\n",
    "\n",
    "optimized_ppo_config = {\n",
    "    'learning_rate': 0.0003,\n",
    "    'n_steps': 4096,         # –ë–æ–ª—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    'batch_size': 256,       # –ë–æ–ª—å—à–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "    'ppo_epochs': 8,         # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n",
    "    'clip_param': 0.1,       # –ú–µ–Ω—å—à–∏–π clip –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "    'entropy_coef': 0.02     # –ë–æ–ª—å—à–µ exploration\n",
    "}\n",
    "```\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ 2: –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**\n",
    "```python\n",
    "# –†–µ—à–µ–Ω–∏–µ: –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "def stable_update(self, loss):\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥\n",
    "    torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN\n",
    "    for param in self.network.parameters():\n",
    "        if torch.isnan(param.grad).any():\n",
    "            print(\"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω NaN –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö!\")\n",
    "            return\n",
    "    \n",
    "    self.optimizer.step()\n",
    "```\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ**\n",
    "```python\n",
    "# –†–µ—à–µ–Ω–∏–µ: Regularization –∏ early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=20, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        \n",
    "    def __call__(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "early_stopping = EarlyStopping(patience=50)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "**–ú–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**\n",
    "```python\n",
    "# –î–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ä–µ–¥\n",
    "def create_multiple_environments(base_port=5005, num_envs=4):\n",
    "    envs = []\n",
    "    for i in range(num_envs):\n",
    "        try:\n",
    "            env = UnityEnvironment(\n",
    "                file_name=\"path/to/environment.exe\",\n",
    "                base_port=base_port + i,\n",
    "                no_graphics=True,  # –û—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞—Ñ–∏–∫—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "                worker_id=i\n",
    "            )\n",
    "            envs.append(env)\n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥—ã {i}: {e}\")\n",
    "    \n",
    "    return envs\n",
    "```\n",
    "\n",
    "**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏**\n",
    "```python\n",
    "# –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞. –°–≤–æ–±–æ–¥–Ω–æ: {torch.cuda.memory_reserved()} bytes\")\n",
    "\n",
    "# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏\n",
    "def monitor_memory():\n",
    "    import psutil\n",
    "    \n",
    "    # RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM: {ram.percent}% –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\")\n",
    "    \n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        print(f\"GPU: {gpu_memory:.2f} GB –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\")\n",
    "\n",
    "# –í—ã–∑–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "monitor_memory()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã\n",
    "\n",
    "**–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**\n",
    "```python\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "def setup_logging(log_level=logging.INFO):\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(f'training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "logger.info(\"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ Unity ML-Agents\")\n",
    "```\n",
    "\n",
    "**Checkpoint —Å–∏—Å—Ç–µ–º–∞**\n",
    "```python\n",
    "class CheckpointManager:\n",
    "    def __init__(self, save_dir=\"checkpoints\"):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, agent, episode, metrics):\n",
    "        checkpoint = {\n",
    "            'episode': episode,\n",
    "            'model_state_dict': agent.network.state_dict(),\n",
    "            'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "            'config': agent.config,\n",
    "            'metrics': metrics,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(self.save_dir, f'checkpoint_episode_{episode}.pth')\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}\")\n",
    "        \n",
    "        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ checkpoints (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5)\n",
    "        self.cleanup_old_checkpoints()\n",
    "    \n",
    "    def cleanup_old_checkpoints(self, keep_last=5):\n",
    "        checkpoints = [f for f in os.listdir(self.save_dir) if f.startswith('checkpoint_')]\n",
    "        if len(checkpoints) > keep_last:\n",
    "            checkpoints.sort()\n",
    "            for checkpoint in checkpoints[:-keep_last]:\n",
    "                os.remove(os.path.join(self.save_dir, checkpoint))\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "checkpoint_manager = CheckpointManager()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω—ã–π framework –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Unity ML-Agents —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–π API —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DQN –∏ PPO –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤. \n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:\n",
    "\n",
    "1. **–ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã** - –í—ã –º–æ–∂–µ—Ç–µ –ª–µ–≥–∫–æ –∏–∑–º–µ–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
    "2. **–î–≤–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–±—É—á–µ–Ω–∏—è** - DQN –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏ PPO –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "3. **–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Unity** - –†–∞–±–æ—Ç–∞ —Å exe-—Ñ–∞–π–ª–∞–º–∏ Unity —Å—Ä–µ–¥—ã —á–µ—Ä–µ–∑ API\n",
    "4. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤** - –ü–æ–¥—Ä–æ–±–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏\n",
    "5. **Troubleshooting** - –†–µ—à–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "\n",
    "1. –ó–∞–º–µ–Ω–∏—Ç–µ mock-—Å—Ä–µ–¥—É –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é Unity —Å—Ä–µ–¥—É\n",
    "2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥ –≤–∞—à—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É\n",
    "3. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ —Å–µ—Ç–µ–π\n",
    "4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ TensorBoard –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "5. –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –∏ –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "### –ü–æ–ª–µ–∑–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã:\n",
    "\n",
    "- [Unity ML-Agents Documentation](https://github.com/Unity-Technologies/ml-agents)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [Stable Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)\n",
    "\n",
    "–£–¥–∞—á–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ –≤–∞—à–∏—Ö Unity ML-Agents! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}