{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML-Agents Complete Training Guide\n",
    "\n",
    "## Comprehensive Step-by-Step Tutorial for Training AI Agents with ONNX Export\n",
    "\n",
    "This notebook provides a complete guide for creating, training, and deploying ML-Agents in Unity with ONNX model export. Each section includes detailed explanations, code examples, and best practices.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Complete Unity ML-Agents setup and installation\n",
    "- Creating and configuring Unity scenes for training\n",
    "- Writing C# agent scripts\n",
    "- Training configuration and hyperparameter tuning\n",
    "- Model training and monitoring\n",
    "- ONNX model export and deployment\n",
    "- Advanced techniques and troubleshooting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Environment Setup\n",
    "\n",
    "### 1.1 Unity Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unity Requirements:**\n",
    "- Unity 2021.3 LTS or newer (recommended)\n",
    "- Windows Build Support or Mac Build Support modules\n",
    "- Unity Hub for easy version management\n",
    "\n",
    "**Installation Steps:**\n",
    "1. Download Unity Hub from [unity.com](https://unity.com/download)\n",
    "2. Install Unity 2021.3 LTS through Unity Hub\n",
    "3. Include platform-specific build modules during installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Python Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version (requires 3.7-3.9)\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Verify version compatibility\n",
    "version_info = sys.version_info\n",
    "if version_info.major == 3 and 7 <= version_info.minor <= 9:\n",
    "    print(\"‚úÖ Python version is compatible with ML-Agents\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Python version may not be fully compatible. Recommended: 3.7-3.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Install Required Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core ML-Agents packages\n",
    "!pip install mlagents torch matplotlib tensorboard onnx\n",
    "\n",
    "# Verify installations\n",
    "import mlagents\n",
    "import torch\n",
    "import matplotlib\n",
    "import onnx\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(f\"ML-Agents version: {mlagents.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.version.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Verify ML-Agents CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ML-Agents command line interface\n",
    "!mlagents-learn --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Unity Project Setup\n",
    "\n",
    "### 2.1 Install ML-Agents Package in Unity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to add ML-Agents to your Unity project:**\n",
    "\n",
    "1. Open your Unity project\n",
    "2. Go to **Window ‚Üí Package Manager**\n",
    "3. Click the **\"+\"** button and select **\"Add package from git URL...\"**\n",
    "4. Enter: `com.unity.ml-agents`\n",
    "5. Click **\"Add\"**\n",
    "\n",
    "**Alternative method (for specific versions):**\n",
    "- Download from [ML-Agents GitHub](https://github.com/Unity-Technologies/ml-agents)\n",
    "- Add as local package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Project Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure for ML-Agents project\n",
    "import os\n",
    "\n",
    "# Define project structure\n",
    "project_dirs = [\n",
    "    \"Unity_ML_Project/Training\",\n",
    "    \"Unity_ML_Project/Training/configs\",\n",
    "    \"Unity_ML_Project/Training/results\",\n",
    "    \"Unity_ML_Project/Models\",\n",
    "    \"Unity_ML_Project/Scripts\"\n",
    "]\n",
    "\n",
    "# Create directories\n",
    "for directory in project_dirs:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"üìÅ Created: {directory}\")\n",
    "\n",
    "print(\"\\n‚úÖ Project structure created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Creating Your First Agent\n",
    "\n",
    "### 3.1 Unity Scene Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new training scene:**\n",
    "\n",
    "1. **File ‚Üí New Scene** and save as `TrainingScene`\n",
    "2. **Add Ground Plane:** GameObject ‚Üí 3D Object ‚Üí Plane\n",
    "3. **Create Training Area:** Empty GameObject named `TrainingArea`\n",
    "4. **Add Agent:** GameObject ‚Üí 3D Object ‚Üí Sphere, name it `Agent`\n",
    "5. **Add Target:** GameObject ‚Üí 3D Object ‚Üí Cube, name it `Target`\n",
    "\n",
    "**Component Setup for Agent:**\n",
    "- Add `Rigidbody` component\n",
    "- Add `Behavior Parameters` component\n",
    "- Add `Decision Requester` component (optional)\n",
    "- Add your custom agent script (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Agent Script (C# for Unity)\n",
    "\n",
    "Create a file called `SimpleAgent.cs` in your Unity project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows the C# script content - copy this to Unity\n",
    "simple_agent_script = '''\n",
    "using Unity.MLAgents;\n",
    "using Unity.MLAgents.Actuators;\n",
    "using Unity.MLAgents.Sensors;\n",
    "using UnityEngine;\n",
    "\n",
    "public class SimpleAgent : Agent\n",
    "{\n",
    "    [Header(\"Agent Settings\")]\n",
    "    public Transform target;\n",
    "    public float moveSpeed = 5f;\n",
    "    public float maxDistance = 5f;\n",
    "    \n",
    "    private Rigidbody agentRb;\n",
    "    private Vector3 startingPosition;\n",
    "    \n",
    "    public override void Initialize()\n",
    "    {\n",
    "        agentRb = GetComponent<Rigidbody>();\n",
    "        startingPosition = transform.localPosition;\n",
    "    }\n",
    "    \n",
    "    public override void OnEpisodeBegin()\n",
    "    {\n",
    "        // Reset agent position and velocity\n",
    "        transform.localPosition = startingPosition;\n",
    "        agentRb.velocity = Vector3.zero;\n",
    "        agentRb.angularVelocity = Vector3.zero;\n",
    "        \n",
    "        // Randomize target position\n",
    "        target.localPosition = new Vector3(\n",
    "            Random.Range(-4f, 4f),\n",
    "            0.5f,\n",
    "            Random.Range(-4f, 4f)\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    public override void CollectObservations(VectorSensor sensor)\n",
    "    {\n",
    "        // Agent position (3 values)\n",
    "        sensor.AddObservation(transform.localPosition);\n",
    "        \n",
    "        // Agent velocity (3 values)\n",
    "        sensor.AddObservation(agentRb.velocity);\n",
    "        \n",
    "        // Target position (3 values)\n",
    "        sensor.AddObservation(target.localPosition);\n",
    "        \n",
    "        // Distance to target (1 value)\n",
    "        sensor.AddObservation(Vector3.Distance(transform.localPosition, target.localPosition));\n",
    "        \n",
    "        // Total observations: 10\n",
    "    }\n",
    "    \n",
    "    public override void OnActionReceived(ActionBuffers actions)\n",
    "    {\n",
    "        // Get continuous actions\n",
    "        float moveX = actions.ContinuousActions[0];\n",
    "        float moveZ = actions.ContinuousActions[1];\n",
    "        \n",
    "        // Apply movement\n",
    "        Vector3 movement = new Vector3(moveX, 0, moveZ);\n",
    "        agentRb.AddForce(movement * moveSpeed);\n",
    "        \n",
    "        // Calculate distance to target\n",
    "        float distanceToTarget = Vector3.Distance(transform.localPosition, target.localPosition);\n",
    "        \n",
    "        // Reward shaping\n",
    "        if (distanceToTarget < 1.5f)\n",
    "        {\n",
    "            // Reached target\n",
    "            SetReward(1.0f);\n",
    "            EndEpisode();\n",
    "        }\n",
    "        else if (transform.localPosition.y < 0)\n",
    "        {\n",
    "            // Fell off platform\n",
    "            SetReward(-1.0f);\n",
    "            EndEpisode();\n",
    "        }\n",
    "        else if (distanceToTarget > maxDistance)\n",
    "        {\n",
    "            // Too far from target\n",
    "            SetReward(-1.0f);\n",
    "            EndEpisode();\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            // Small reward for getting closer\n",
    "            SetReward(-0.01f + (1f / distanceToTarget) * 0.01f);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public override void Heuristic(in ActionBuffers actionsOut)\n",
    "    {\n",
    "        // Manual control for testing\n",
    "        var continuousActions = actionsOut.ContinuousActions;\n",
    "        continuousActions[0] = Input.GetAxis(\"Horizontal\");\n",
    "        continuousActions[1] = Input.GetAxis(\"Vertical\");\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Save the script to file\n",
    "with open('Unity_ML_Project/Scripts/SimpleAgent.cs', 'w') as f:\n",
    "    f.write(simple_agent_script)\n",
    "\n",
    "print(\"‚úÖ SimpleAgent.cs script created!\")\n",
    "print(\"üìã Copy this script to your Unity project's Scripts folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Behavior Parameters Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure the Behavior Parameters component:**\n",
    "\n",
    "1. **Behavior Name:** `SimpleAgent` (must match YAML config)\n",
    "2. **Vector Observation ‚Üí Space Size:** `10` (from our CollectObservations)\n",
    "3. **Actions ‚Üí Continuous Actions ‚Üí Space Size:** `2` (X and Z movement)\n",
    "4. **Behavior Type:** `Default` (for training) or `Heuristic Only` (for manual testing)\n",
    "\n",
    "**Decision Requester Settings:**\n",
    "- **Decision Period:** `5` (agent makes decisions every 5 frames)\n",
    "- **Take Actions Between Decisions:** `true`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Training Configuration\n",
    "\n",
    "### 4.1 Basic Training Configuration (YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration\n",
    "basic_config = '''\n",
    "behaviors:\n",
    "  SimpleAgent:\n",
    "    trainer_type: ppo\n",
    "    hyperparameters:\n",
    "      batch_size: 128\n",
    "      buffer_size: 2048\n",
    "      learning_rate: 3.0e-4\n",
    "      beta: 5.0e-4\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.99\n",
    "      num_epoch: 3\n",
    "      learning_rate_schedule: linear\n",
    "    network_settings:\n",
    "      normalize: false\n",
    "      hidden_units: 128\n",
    "      num_layers: 2\n",
    "      vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "    max_steps: 500000\n",
    "    time_horizon: 64\n",
    "    summary_freq: 10000\n",
    "    keep_checkpoints: 5\n",
    "'''\n",
    "\n",
    "# Save configuration\n",
    "with open('Unity_ML_Project/Training/configs/simple_agent_config.yaml', 'w') as f:\n",
    "    f.write(basic_config)\n",
    "\n",
    "print(\"‚úÖ Basic training configuration created!\")\n",
    "print(\"üìÅ Saved to: Unity_ML_Project/Training/configs/simple_agent_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Advanced Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced configuration with more options\n",
    "advanced_config = '''\n",
    "behaviors:\n",
    "  SimpleAgent:\n",
    "    trainer_type: ppo\n",
    "    hyperparameters:\n",
    "      batch_size: 256\n",
    "      buffer_size: 10240\n",
    "      learning_rate: 3.0e-4\n",
    "      beta: 5.0e-4\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.99\n",
    "      num_epoch: 3\n",
    "      learning_rate_schedule: linear\n",
    "    network_settings:\n",
    "      normalize: true\n",
    "      hidden_units: 256\n",
    "      num_layers: 3\n",
    "      vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "      curiosity:\n",
    "        gamma: 0.99\n",
    "        strength: 0.02\n",
    "        network_settings:\n",
    "          normalize: false\n",
    "          hidden_units: 64\n",
    "          num_layers: 2\n",
    "        learning_rate: 3.0e-4\n",
    "    behavioral_cloning:\n",
    "      demo_path: ./demonstrations/SimpleAgent.demo\n",
    "      strength: 0.5\n",
    "      steps: 150000\n",
    "    max_steps: 1000000\n",
    "    time_horizon: 128\n",
    "    summary_freq: 10000\n",
    "    keep_checkpoints: 10\n",
    "    checkpoint_interval: 50000\n",
    "    threaded: false\n",
    "'''\n",
    "\n",
    "# Save advanced configuration\n",
    "with open('Unity_ML_Project/Training/configs/advanced_agent_config.yaml', 'w') as f:\n",
    "    f.write(advanced_config)\n",
    "\n",
    "print(\"‚úÖ Advanced training configuration created!\")\n",
    "print(\"üìÅ Saved to: Unity_ML_Project/Training/configs/advanced_agent_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SAC (Soft Actor-Critic) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SAC configuration for continuous control tasks\n",
    "sac_config = '''\n",
    "behaviors:\n",
    "  SimpleAgent:\n",
    "    trainer_type: sac\n",
    "    hyperparameters:\n",
    "      learning_rate: 3.0e-4\n",
    "      learning_rate_schedule: constant\n",
    "      batch_size: 128\n",
    "      buffer_size: 50000\n",
    "      buffer_init_steps: 0\n",
    "      tau: 0.005\n",
    "      steps_per_update: 1\n",
    "      save_replay_buffer: false\n",
    "      init_entcoef: 1.0\n",
    "      reward_signal_steps_per_update: 1\n",
    "    network_settings:\n",
    "      normalize: false\n",
    "      hidden_units: 256\n",
    "      num_layers: 2\n",
    "      vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "    max_steps: 500000\n",
    "    time_horizon: 64\n",
    "    summary_freq: 10000\n",
    "    keep_checkpoints: 5\n",
    "'''\n",
    "\n",
    "# Save SAC configuration\n",
    "with open('Unity_ML_Project/Training/configs/sac_agent_config.yaml', 'w') as f:\n",
    "    f.write(sac_config)\n",
    "\n",
    "print(\"‚úÖ SAC training configuration created!\")\n",
    "print(\"üìÅ Saved to: Unity_ML_Project/Training/configs/sac_agent_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training Execution\n",
    "\n",
    "### 5.1 Pre-Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training environment\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_training_environment():\n",
    "    \"\"\"Setup directories and environment for training\"\"\"\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = \"Unity_ML_Project/Training/results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Create logs directory\n",
    "    logs_dir = \"Unity_ML_Project/Training/logs\"\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"‚úÖ Training environment setup complete\")\n",
    "    print(f\"üìÅ Results directory: {results_dir}\")\n",
    "    print(f\"üìÅ Logs directory: {logs_dir}\")\n",
    "    \n",
    "    return results_dir, logs_dir\n",
    "\n",
    "results_dir, logs_dir = setup_training_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training commands\n",
    "def generate_training_commands():\n",
    "    \"\"\"Generate different training command examples\"\"\"\n",
    "    \n",
    "    commands = {\n",
    "        \"basic_training\": {\n",
    "            \"description\": \"Basic training with Unity Editor\",\n",
    "            \"command\": \"mlagents-learn Unity_ML_Project/Training/configs/simple_agent_config.yaml --run-id=basic_run --train\"\n",
    "        },\n",
    "        \"build_training\": {\n",
    "            \"description\": \"Training with built executable\",\n",
    "            \"command\": \"mlagents-learn Unity_ML_Project/Training/configs/simple_agent_config.yaml --run-id=build_run --env=path/to/your/build.exe --train\"\n",
    "        },\n",
    "        \"resume_training\": {\n",
    "            \"description\": \"Resume previous training\",\n",
    "            \"command\": \"mlagents-learn Unity_ML_Project/Training/configs/simple_agent_config.yaml --run-id=basic_run --resume --train\"\n",
    "        },\n",
    "        \"advanced_training\": {\n",
    "            \"description\": \"Advanced training with custom parameters\",\n",
    "            \"command\": \"mlagents-learn Unity_ML_Project/Training/configs/advanced_agent_config.yaml --run-id=advanced_run --train --num-envs=4 --width=1920 --height=1080\"\n",
    "        },\n",
    "        \"sac_training\": {\n",
    "            \"description\": \"SAC algorithm training\",\n",
    "            \"command\": \"mlagents-learn Unity_ML_Project/Training/configs/sac_agent_config.yaml --run-id=sac_run --train\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Training Commands:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, info in commands.items():\n",
    "        print(f\"\\nüìã {info['description']}:\")\n",
    "        print(f\"   {info['command']}\")\n",
    "    \n",
    "    return commands\n",
    "\n",
    "training_commands = generate_training_commands()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Training Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard monitoring setup\n",
    "def setup_tensorboard_monitoring():\n",
    "    \"\"\"Setup TensorBoard for training monitoring\"\"\"\n",
    "    \n",
    "    tensorboard_command = \"tensorboard --logdir=Unity_ML_Project/Training/results --port=6006\"\n",
    "    \n",
    "    print(\"üìä TensorBoard Monitoring Setup:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Command: {tensorboard_command}\")\n",
    "    print(\"\\nüìà Key Metrics to Monitor:\")\n",
    "    print(\"- Environment/Cumulative Reward\")\n",
    "    print(\"- Environment/Episode Length\")\n",
    "    print(\"- Policy/Learning Rate\")\n",
    "    print(\"- Policy/Entropy\")\n",
    "    print(\"- Policy/Value Loss\")\n",
    "    print(\"- Policy/Policy Loss\")\n",
    "    \n",
    "    print(\"\\nüåê Access TensorBoard at: http://localhost:6006\")\n",
    "    \n",
    "    return tensorboard_command\n",
    "\n",
    "tensorboard_cmd = setup_tensorboard_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Training Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training progress analysis\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_training_progress(run_id=\"basic_run\"):\n",
    "    \"\"\"Analyze training progress from results\"\"\"\n",
    "    \n",
    "    # This function would typically read from actual training results\n",
    "    # For demonstration, we'll create sample data\n",
    "    \n",
    "    # Sample training data\n",
    "    steps = np.arange(0, 100000, 1000)\n",
    "    rewards = np.random.normal(0.5, 0.3, len(steps)).cumsum() * 0.01\n",
    "    episode_lengths = np.random.normal(100, 20, len(steps))\n",
    "    \n",
    "    # Create plots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Reward plot\n",
    "    ax1.plot(steps, rewards, 'b-', linewidth=2)\n",
    "    ax1.set_title('Training Progress: Cumulative Reward')\n",
    "    ax1.set_xlabel('Training Steps')\n",
    "    ax1.set_ylabel('Cumulative Reward')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Episode length plot\n",
    "    ax2.plot(steps, episode_lengths, 'r-', linewidth=2)\n",
    "    ax2.set_title('Training Progress: Episode Length')\n",
    "    ax2.set_xlabel('Training Steps')\n",
    "    ax2.set_ylabel('Episode Length')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Unity_ML_Project/Training/training_progress.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Training statistics\n",
    "    stats = {\n",
    "        \"final_reward\": float(rewards[-1]),\n",
    "        \"max_reward\": float(np.max(rewards)),\n",
    "        \"avg_episode_length\": float(np.mean(episode_lengths)),\n",
    "        \"total_steps\": int(steps[-1])\n",
    "    }\n",
    "    \n",
    "    print(\"üìä Training Statistics:\")\n",
    "    print(f\"   Final Reward: {stats['final_reward']:.3f}\")\n",
    "    print(f\"   Max Reward: {stats['max_reward']:.3f}\")\n",
    "    print(f\"   Avg Episode Length: {stats['avg_episode_length']:.1f}\")\n",
    "    print(f\"   Total Steps: {stats['total_steps']:,}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run analysis\n",
    "training_stats = analyze_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ONNX Model Export and Management\n",
    "\n",
    "### 6.1 ONNX Export Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX model export utilities\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import shutil\n",
    "\n",
    "def export_model_to_onnx(run_id=\"basic_run\", behavior_name=\"SimpleAgent\"):\n",
    "    \"\"\"Export trained model to ONNX format\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    results_path = f\"Unity_ML_Project/Training/results/{run_id}\"\n",
    "    onnx_path = f\"Unity_ML_Project/Models/{behavior_name}_{run_id}.onnx\"\n",
    "    \n",
    "    print(f\"üîÑ Exporting model to ONNX format...\")\n",
    "    print(f\"   Source: {results_path}\")\n",
    "    print(f\"   Target: {onnx_path}\")\n",
    "    \n",
    "    # ML-Agents automatically generates ONNX files during training\n",
    "    # The ONNX file will be in the results directory\n",
    "    source_onnx = f\"{results_path}/{behavior_name}.onnx\"\n",
    "    \n",
    "    try:\n",
    "        # Copy ONNX file to Models directory\n",
    "        shutil.copy2(source_onnx, onnx_path)\n",
    "        print(f\"‚úÖ Model exported successfully!\")\n",
    "        \n",
    "        # Verify ONNX model\n",
    "        model = onnx.load(onnx_path)\n",
    "        onnx.checker.check_model(model)\n",
    "        print(f\"‚úÖ ONNX model validation passed!\")\n",
    "        \n",
    "        return onnx_path\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è ONNX file not found at {source_onnx}\")\n",
    "        print(f\"   Make sure training completed successfully\")\n",
    "        return None\n",
    "\n",
    "def analyze_onnx_model(onnx_path):\n",
    "    \"\"\"Analyze ONNX model structure\"\"\"\n",
    "    \n",
    "    try:\n",
    "        model = onnx.load(onnx_path)\n",
    "        \n",
    "        print(f\"üîç ONNX Model Analysis:\")\n",
    "        print(f\"   Model version: {model.model_version}\")\n",
    "        print(f\"   Producer: {model.producer_name} {model.producer_version}\")\n",
    "        print(f\"   Graph name: {model.graph.name}\")\n",
    "        \n",
    "        # Input information\n",
    "        print(f\"\\nüì• Model Inputs:\")\n",
    "        for inp in model.graph.input:\n",
    "            shape = [dim.dim_value for dim in inp.type.tensor_type.shape.dim]\n",
    "            print(f\"   - {inp.name}: {shape}\")\n",
    "        \n",
    "        # Output information\n",
    "        print(f\"\\nüì§ Model Outputs:\")\n",
    "        for out in model.graph.output:\n",
    "            shape = [dim.dim_value for dim in out.type.tensor_type.shape.dim]\n",
    "            print(f\"   - {out.name}: {shape}\")\n",
    "        \n",
    "        # Node count\n",
    "        print(f\"\\nüß† Model Structure:\")\n",
    "        print(f\"   Total nodes: {len(model.graph.node)}\")\n",
    "        \n",
    "        # Node types\n",
    "        node_types = {}\n",
    "        for node in model.graph.node:\n",
    "            node_types[node.op_type] = node_types.get(node.op_type, 0) + 1\n",
    "        \n",
    "        print(f\"   Node types:\")\n",
    "        for op_type, count in sorted(node_types.items()):\n",
    "            print(f\"     - {op_type}: {count}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing ONNX model: {e}\")\n",
    "\n",
    "# Example usage (uncomment when you have trained models)\n",
    "# onnx_model_path = export_model_to_onnx(\"basic_run\", \"SimpleAgent\")\n",
    "# if onnx_model_path:\n",
    "#     analyze_onnx_model(onnx_model_path)\n",
    "\n",
    "print(\"‚úÖ ONNX export utilities ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ONNX Runtime Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ONNX model with ONNX Runtime\n",
    "def test_onnx_model(onnx_path, test_input=None):\n",
    "    \"\"\"Test ONNX model inference\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load ONNX Runtime session\n",
    "        session = ort.InferenceSession(onnx_path)\n",
    "        \n",
    "        print(f\"üß™ Testing ONNX model: {onnx_path}\")\n",
    "        \n",
    "        # Get model input/output info\n",
    "        input_info = session.get_inputs()[0]\n",
    "        output_info = session.get_outputs()[0]\n",
    "        \n",
    "        print(f\"   Input: {input_info.name} {input_info.shape}\")\n",
    "        print(f\"   Output: {output_info.name} {output_info.shape}\")\n",
    "        \n",
    "        # Create test input if not provided\n",
    "        if test_input is None:\n",
    "            input_shape = input_info.shape\n",
    "            if -1 in input_shape:  # Dynamic batch size\n",
    "                input_shape = [1 if dim == -1 else dim for dim in input_shape]\n",
    "            \n",
    "            test_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "            print(f\"   Generated test input shape: {test_input.shape}\")\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = session.run([output_info.name], {input_info.name: test_input})\n",
    "        \n",
    "        print(f\"‚úÖ Inference successful!\")\n",
    "        print(f\"   Output shape: {outputs[0].shape}\")\n",
    "        print(f\"   Output sample: {outputs[0].flatten()[:5]}\")\n",
    "        \n",
    "        return outputs[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing ONNX model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "print(\"‚úÖ ONNX testing utilities ready!\")\n",
    "print(\"   Use test_onnx_model('path/to/model.onnx') to test your models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Unity ONNX Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unity integration instructions for ONNX models\n",
    "unity_integration_guide = '''\n",
    "# Unity ONNX Model Integration Guide\n",
    "\n",
    "## Step 1: Import ONNX Model to Unity\n",
    "1. Drag your .onnx file into Unity's Assets folder\n",
    "2. Unity will automatically import it as a Model asset\n",
    "3. Recommended location: Assets/ML-Agents/Models/\n",
    "\n",
    "## Step 2: Configure Behavior Parameters\n",
    "1. Select your Agent GameObject\n",
    "2. In Behavior Parameters component:\n",
    "   - Set \"Behavior Type\" to \"Inference Only\"\n",
    "   - Drag your .onnx file to the \"Model\" field\n",
    "   - Ensure \"Use Child Sensors\" is checked if using sensors\n",
    "\n",
    "## Step 3: Disable Training Components\n",
    "1. Remove or disable \"Decision Requester\" component\n",
    "2. Your agent will now use the trained model for decisions\n",
    "\n",
    "## Step 4: Test Model Performance\n",
    "1. Enter Play mode\n",
    "2. Observe agent behavior\n",
    "3. Monitor performance in Profiler if needed\n",
    "\n",
    "## Troubleshooting:\n",
    "- Ensure observation space matches training configuration\n",
    "- Check action space dimensions\n",
    "- Verify normalization settings match training\n",
    "- Test with Heuristic mode first to validate environment\n",
    "'''\n",
    "\n",
    "# Save integration guide\n",
    "with open('Unity_ML_Project/Unity_ONNX_Integration_Guide.md', 'w') as f:\n",
    "    f.write(unity_integration_guide)\n",
    "\n",
    "print(\"‚úÖ Unity ONNX integration guide created!\")\n",
    "print(\"üìÅ Saved to: Unity_ML_Project/Unity_ONNX_Integration_Guide.md\")\n",
    "print(\"\\nüìã Integration Steps:\")\n",
    "print(unity_integration_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Advanced Training Techniques\n",
    "\n",
    "### 7.1 Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curriculum learning configuration\n",
    "curriculum_config = '''\n",
    "behaviors:\n",
    "  SimpleAgent:\n",
    "    trainer_type: ppo\n",
    "    hyperparameters:\n",
    "      batch_size: 128\n",
    "      buffer_size: 2048\n",
    "      learning_rate: 3.0e-4\n",
    "      beta: 5.0e-4\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.99\n",
    "      num_epoch: 3\n",
    "    network_settings:\n",
    "      normalize: false\n",
    "      hidden_units: 128\n",
    "      num_layers: 2\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "    max_steps: 500000\n",
    "    time_horizon: 64\n",
    "    summary_freq: 10000\n",
    "    keep_checkpoints: 5\n",
    "\n",
    "# Curriculum Learning Configuration\n",
    "curriculum:\n",
    "  SimpleAgent:\n",
    "    measure: progress\n",
    "    thresholds: [0.1, 0.3, 0.5]\n",
    "    min_lesson_length: 100\n",
    "    signal_smoothing: true\n",
    "    parameters:\n",
    "      target_distance:\n",
    "        curriculum:\n",
    "          - name: lesson1\n",
    "            completion_criteria:\n",
    "              measure: progress\n",
    "              behavior: SimpleAgent\n",
    "              signal_smoothing: true\n",
    "              min_lesson_length: 100\n",
    "              threshold: 0.2\n",
    "            value: 2.0\n",
    "          - name: lesson2\n",
    "            completion_criteria:\n",
    "              measure: progress\n",
    "              behavior: SimpleAgent\n",
    "              signal_smoothing: true\n",
    "              min_lesson_length: 100\n",
    "              threshold: 0.5\n",
    "            value: 4.0\n",
    "          - name: lesson3\n",
    "            value: 6.0\n",
    "'''\n",
    "\n",
    "# Save curriculum configuration\n",
    "with open('Unity_ML_Project/Training/configs/curriculum_config.yaml', 'w') as f:\n",
    "    f.write(curriculum_config)\n",
    "\n",
    "print(\"‚úÖ Curriculum learning configuration created!\")\n",
    "print(\"üìÅ Saved to: Unity_ML_Project/Training/configs/curriculum_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Multi-Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-agent training configuration\n",
    "multiagent_config = '''\n",
    "behaviors:\n",
    "  SimpleAgent:\n",
    "    trainer_type: ppo\n",
    "    hyperparameters:\n",
    "      batch_size: 512\n",
    "      buffer_size: 10240\n",
    "      learning_rate: 3.0e-4\n",
    "      beta: 5.0e-4\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.99\n",
    "      num_epoch: 3\n",
    "    network_settings:\n",
    "      normalize: false\n",
    "      hidden_units: 256\n",
    "      num_layers: 2\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "    max_steps: 1000000\n",
    "    time_horizon: 128\n",
    "    summary_freq: 10000\n",
    "    keep_checkpoints: 5\n",
    "    \n",
    "  CompetitorAgent:\n",
    "    trainer_type: ppo\n",
    "    hyperparameters:\n",
    "      batch_size: 512\n",
    "      buffer_size: 10240\n",
    "      learning_rate: 3.0e-4\n",
    "      beta: 5.0e-4\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.99\n",
    "      num_epoch: 3\n",
    "    network_settings:\n",
    "      normalize: false\n",
    "      hidden_units: 256\n",
    "      num_layers: 2\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "    max_steps: 1000000\n",
    "    time_horizon: 128\n",
    "    summary_freq: 10000\n",
    "    keep_checkpoints: 5\n",
    "\n",
    "# Self-play configuration\n",
    "env_settings:\n",
    "  env_path: ./builds/MultiAgent\n",
    "  env_args: null\n",
    "  base_port: 5005\n",
    "  num_envs: 1\n",
    "  num_areas: 1\n",
    "  seed: -1\n",
    "  max_lifetime_restarts: 10\n",
    "  restarts_rate_limit_n: 1\n",
    "  restarts_rate_limit_period_s: 60\n",
    "'''\n",
    "\n",
    "# Save multi-agent configuration\n",
    "with open('Unity_ML_Project/Training/configs/multiagent_config.yaml', 'w') as f:\n",
    "    f.write(multiagent_config)\n",
    "\n",
    "print(\"‚úÖ Multi-agent training configuration created!\")\n",
    "print(\"üìÅ Saved to: Unity_ML_Project/Training/configs/multiagent_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization utilities\n",
    "import itertools\n",
    "import yaml\n",
    "\n",
    "def generate_hyperparameter_sweep():\n",
    "    \"\"\"Generate multiple configurations for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Define parameter ranges\n",
    "    param_ranges = {\n",
    "        'learning_rate': [1e-4, 3e-4, 1e-3],\n",
    "        'batch_size': [64, 128, 256],\n",
    "        'hidden_units': [128, 256, 512],\n",
    "        'num_layers': [2, 3, 4],\n",
    "        'buffer_size': [2048, 10240, 20480]\n",
    "    }\n",
    "    \n",
    "    # Base configuration\n",
    "    base_config = {\n",
    "        'behaviors': {\n",
    "            'SimpleAgent': {\n",
    "                'trainer_type': 'ppo',\n",
    "                'hyperparameters': {\n",
    "                    'beta': 5.0e-4,\n",
    "                    'epsilon': 0.2,\n",
    "                    'lambd': 0.99,\n",
    "                    'num_epoch': 3\n",
    "                },\n",
    "                'network_settings': {\n",
    "                    'normalize': False\n",
    "                },\n",
    "                'reward_signals': {\n",
    "                    'extrinsic': {\n",
    "                        'gamma': 0.99,\n",
    "                        'strength': 1.0\n",
    "                    }\n",
    "                },\n",
    "                'max_steps': 200000,\n",
    "                'time_horizon': 64,\n",
    "                'summary_freq': 10000,\n",
    "                'keep_checkpoints': 3\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generate limited combinations (avoid combinatorial explosion)\n",
    "    configs = []\n",
    "    config_id = 0\n",
    "    \n",
    "    # Sample a few combinations instead of all\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    \n",
    "    for lr in param_ranges['learning_rate']:\n",
    "        for batch_size in param_ranges['batch_size']:\n",
    "            for hidden_units in param_ranges['hidden_units'][:2]:  # Limit to reduce combinations\n",
    "                config = base_config.copy()\n",
    "                config['behaviors']['SimpleAgent']['hyperparameters']['learning_rate'] = lr\n",
    "                config['behaviors']['SimpleAgent']['hyperparameters']['batch_size'] = batch_size\n",
    "                config['behaviors']['SimpleAgent']['network_settings']['hidden_units'] = hidden_units\n",
    "                \n",
    "                # Save configuration\n",
    "                config_filename = f'Unity_ML_Project/Training/configs/sweep_config_{config_id:03d}.yaml'\n",
    "                with open(config_filename, 'w') as f:\n",
    "                    yaml.dump(config, f, default_flow_style=False)\n",
    "                \n",
    "                configs.append({\n",
    "                    'id': config_id,\n",
    "                    'filename': config_filename,\n",
    "                    'params': {\n",
    "                        'learning_rate': lr,\n",
    "                        'batch_size': batch_size,\n",
    "                        'hidden_units': hidden_units\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                config_id += 1\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(configs)} hyperparameter configurations\")\n",
    "    \n",
    "    # Generate training script\n",
    "    training_script = \"#!/bin/bash\\n\\n\"\n",
    "    training_script += \"# Hyperparameter sweep training script\\n\\n\"\n",
    "    \n",
    "    for config in configs:\n",
    "        run_id = f\"sweep_run_{config['id']:03d}\"\n",
    "        training_script += f\"echo 'Starting training {config['id']}: {config['params']}'\\n\"\n",
    "        training_script += f\"mlagents-learn {config['filename']} --run-id={run_id} --train\\n\\n\"\n",
    "    \n",
    "    with open('Unity_ML_Project/Training/run_hyperparameter_sweep.sh', 'w') as f:\n",
    "        f.write(training_script)\n",
    "    \n",
    "    print(\"‚úÖ Hyperparameter sweep script created!\")\n",
    "    print(\"üìÅ Saved to: Unity_ML_Project/Training/run_hyperparameter_sweep.sh\")\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Generate sweep configurations\n",
    "sweep_configs = generate_hyperparameter_sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Model Comparison and Analysis\n",
    "\n",
    "### 8.1 Training Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison utilities\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def compare_training_results(run_ids):\n",
    "    \"\"\"Compare results from multiple training runs\"\"\"\n",
    "    \n",
    "    # Sample data for demonstration\n",
    "    comparison_data = []\n",
    "    \n",
    "    for i, run_id in enumerate(run_ids):\n",
    "        # In real scenario, you'd load actual training data\n",
    "        # Here we generate sample data\n",
    "        final_reward = np.random.normal(0.8, 0.2)\n",
    "        max_reward = final_reward + np.random.uniform(0.1, 0.3)\n",
    "        training_time = np.random.uniform(30, 120)  # minutes\n",
    "        convergence_step = np.random.randint(50000, 200000)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'run_id': run_id,\n",
    "            'final_reward': final_reward,\n",
    "            'max_reward': max_reward,\n",
    "            'training_time_min': training_time,\n",
    "            'convergence_step': convergence_step,\n",
    "            'efficiency': final_reward / training_time\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Final reward comparison\n",
    "    axes[0, 0].bar(df['run_id'], df['final_reward'], color='skyblue')\n",
    "    axes[0, 0].set_title('Final Reward Comparison')\n",
    "    axes[0, 0].set_ylabel('Final Reward')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Training time comparison\n",
    "    axes[0, 1].bar(df['run_id'], df['training_time_min'], color='lightcoral')\n",
    "    axes[0, 1].set_title('Training Time Comparison')\n",
    "    axes[0, 1].set_ylabel('Training Time (minutes)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Convergence comparison\n",
    "    axes[1, 0].bar(df['run_id'], df['convergence_step'], color='lightgreen')\n",
    "    axes[1, 0].set_title('Convergence Speed')\n",
    "    axes[1, 0].set_ylabel('Steps to Convergence')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Efficiency scatter\n",
    "    axes[1, 1].scatter(df['training_time_min'], df['final_reward'], s=100, alpha=0.7)\n",
    "    axes[1, 1].set_title('Training Efficiency')\n",
    "    axes[1, 1].set_xlabel('Training Time (minutes)')\n",
    "    axes[1, 1].set_ylabel('Final Reward')\n",
    "    \n",
    "    # Add run_id labels to scatter plot\n",
    "    for i, run_id in enumerate(df['run_id']):\n",
    "        axes[1, 1].annotate(run_id, (df['training_time_min'].iloc[i], df['final_reward'].iloc[i]),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Unity_ML_Project/Training/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"üìä Training Results Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.round(3).to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_reward_idx = df['final_reward'].idxmax()\n",
    "    best_efficiency_idx = df['efficiency'].idxmax()\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Performance:\")\n",
    "    print(f\"   Highest Reward: {df.loc[best_reward_idx, 'run_id']} ({df.loc[best_reward_idx, 'final_reward']:.3f})\")\n",
    "    print(f\"   Best Efficiency: {df.loc[best_efficiency_idx, 'run_id']} ({df.loc[best_efficiency_idx, 'efficiency']:.4f})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example comparison\n",
    "sample_runs = ['basic_run', 'advanced_run', 'sac_run', 'curriculum_run']\n",
    "comparison_results = compare_training_results(sample_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Model Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking utilities\n",
    "import time\n",
    "\n",
    "def benchmark_onnx_inference(onnx_path, num_inferences=1000):\n",
    "    \"\"\"Benchmark ONNX model inference speed\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        session = ort.InferenceSession(onnx_path)\n",
    "        input_info = session.get_inputs()[0]\n",
    "        \n",
    "        # Prepare test input\n",
    "        input_shape = input_info.shape\n",
    "        if -1 in input_shape:\n",
    "            input_shape = [1 if dim == -1 else dim for dim in input_shape]\n",
    "        \n",
    "        test_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "        \n",
    "        print(f\"üöÄ Benchmarking ONNX model: {onnx_path}\")\n",
    "        print(f\"   Input shape: {input_shape}\")\n",
    "        print(f\"   Number of inferences: {num_inferences}\")\n",
    "        \n",
    "        # Warm-up runs\n",
    "        for _ in range(10):\n",
    "            session.run(None, {input_info.name: test_input})\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_inferences):\n",
    "            outputs = session.run(None, {input_info.name: test_input})\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_time = end_time - start_time\n",
    "        avg_inference_time = (total_time / num_inferences) * 1000  # ms\n",
    "        inferences_per_second = num_inferences / total_time\n",
    "        \n",
    "        print(f\"\\nüìä Benchmark Results:\")\n",
    "        print(f\"   Total time: {total_time:.3f} seconds\")\n",
    "        print(f\"   Average inference time: {avg_inference_time:.3f} ms\")\n",
    "        print(f\"   Inferences per second: {inferences_per_second:.1f}\")\n",
    "        \n",
    "        # Performance rating\n",
    "        if avg_inference_time < 1.0:\n",
    "            rating = \"üü¢ Excellent (< 1ms)\"\n",
    "        elif avg_inference_time < 5.0:\n",
    "            rating = \"üü° Good (1-5ms)\"\n",
    "        elif avg_inference_time < 10.0:\n",
    "            rating = \"üü† Acceptable (5-10ms)\"\n",
    "        else:\n",
    "            rating = \"üî¥ Slow (> 10ms)\"\n",
    "        \n",
    "        print(f\"   Performance rating: {rating}\")\n",
    "        \n",
    "        return {\n",
    "            'total_time': total_time,\n",
    "            'avg_inference_time_ms': avg_inference_time,\n",
    "            'inferences_per_second': inferences_per_second\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Benchmarking failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Memory usage analysis\n",
    "def analyze_model_size(onnx_path):\n",
    "    \"\"\"Analyze ONNX model size and complexity\"\"\"\n",
    "    \n",
    "    try:\n",
    "        import os\n",
    "        \n",
    "        # File size\n",
    "        file_size_bytes = os.path.getsize(onnx_path)\n",
    "        file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "        \n",
    "        # Load model for analysis\n",
    "        model = onnx.load(onnx_path)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = 0\n",
    "        for initializer in model.graph.initializer:\n",
    "            param_size = 1\n",
    "            for dim in initializer.dims:\n",
    "                param_size *= dim\n",
    "            total_params += param_size\n",
    "        \n",
    "        print(f\"üìè Model Size Analysis:\")\n",
    "        print(f\"   File size: {file_size_mb:.2f} MB ({file_size_bytes:,} bytes)\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Model nodes: {len(model.graph.node)}\")\n",
    "        \n",
    "        # Size category\n",
    "        if file_size_mb < 1:\n",
    "            size_category = \"üü¢ Small (< 1MB)\"\n",
    "        elif file_size_mb < 10:\n",
    "            size_category = \"üü° Medium (1-10MB)\"\n",
    "        elif file_size_mb < 100:\n",
    "            size_category = \"üü† Large (10-100MB)\"\n",
    "        else:\n",
    "            size_category = \"üî¥ Very Large (> 100MB)\"\n",
    "        \n",
    "        print(f\"   Size category: {size_category}\")\n",
    "        \n",
    "        return {\n",
    "            'file_size_mb': file_size_mb,\n",
    "            'total_parameters': total_params,\n",
    "            'node_count': len(model.graph.node)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Size analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Benchmarking utilities ready!\")\n",
    "print(\"   Use benchmark_onnx_inference('path/to/model.onnx') to test performance\")\n",
    "print(\"   Use analyze_model_size('path/to/model.onnx') to analyze model size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Troubleshooting and Best Practices\n",
    "\n",
    "### 9.1 Common Issues and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting guide\n",
    "troubleshooting_guide = '''\n",
    "# Unity ML-Agents Troubleshooting Guide\n",
    "\n",
    "## 1. Training Issues\n",
    "\n",
    "### Problem: Training doesn't start\n",
    "**Symptoms:**\n",
    "- Command hangs after \"Start training by pressing the Play button in the Unity Editor\"\n",
    "- No connection between Unity and Python\n",
    "\n",
    "**Solutions:**\n",
    "- Ensure Unity is in Play mode\n",
    "- Check that Behavior Name in Unity matches YAML config\n",
    "- Verify no firewall blocking communication\n",
    "- Try different port with --base-port parameter\n",
    "\n",
    "### Problem: Slow or no learning\n",
    "**Symptoms:**\n",
    "- Reward stays flat or decreases\n",
    "- Agent behavior doesn't improve\n",
    "\n",
    "**Solutions:**\n",
    "- Review reward function design\n",
    "- Check observation space completeness\n",
    "- Adjust learning rate (try 1e-4 to 1e-3)\n",
    "- Increase training steps\n",
    "- Use reward shaping techniques\n",
    "\n",
    "### Problem: Training crashes\n",
    "**Symptoms:**\n",
    "- Python process terminates unexpectedly\n",
    "- Out of memory errors\n",
    "\n",
    "**Solutions:**\n",
    "- Reduce batch_size and buffer_size\n",
    "- Decrease number of parallel environments\n",
    "- Check for infinite loops in agent code\n",
    "- Monitor system resources\n",
    "\n",
    "## 2. ONNX Export Issues\n",
    "\n",
    "### Problem: ONNX file not generated\n",
    "**Solutions:**\n",
    "- Ensure training completed successfully\n",
    "- Check results directory permissions\n",
    "- Verify ONNX package installation\n",
    "\n",
    "### Problem: ONNX model doesn't work in Unity\n",
    "**Solutions:**\n",
    "- Verify observation space matches exactly\n",
    "- Check action space configuration\n",
    "- Ensure normalization settings match\n",
    "- Test with Heuristic mode first\n",
    "\n",
    "## 3. Performance Issues\n",
    "\n",
    "### Problem: Slow inference in Unity\n",
    "**Solutions:**\n",
    "- Reduce network size (hidden_units, num_layers)\n",
    "- Optimize observation collection\n",
    "- Use Decision Requester with appropriate frequency\n",
    "- Profile with Unity Profiler\n",
    "\n",
    "## 4. Configuration Issues\n",
    "\n",
    "### Problem: YAML parsing errors\n",
    "**Solutions:**\n",
    "- Check YAML syntax (indentation, colons)\n",
    "- Validate with online YAML validator\n",
    "- Use consistent spacing (2 or 4 spaces)\n",
    "\n",
    "### Problem: Hyperparameter tuning difficulties\n",
    "**Solutions:**\n",
    "- Start with proven configurations\n",
    "- Change one parameter at a time\n",
    "- Use curriculum learning for complex tasks\n",
    "- Monitor TensorBoard metrics\n",
    "'''\n",
    "\n",
    "# Save troubleshooting guide\n",
    "with open('Unity_ML_Project/Troubleshooting_Guide.md', 'w') as f:\n",
    "    f.write(troubleshooting_guide)\n",
    "\n",
    "print(\"‚úÖ Troubleshooting guide created!\")\n",
    "print(\"üìÅ Saved to: Unity_ML_Project/Troubleshooting_Guide.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Best Practices Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices checklist\n",
    "def generate_best_practices_checklist():\n",
    "    \"\"\"Generate comprehensive best practices checklist\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"Environment Design\": [\n",
    "            \"‚úì Clear success/failure conditions defined\",\n",
    "            \"‚úì Appropriate episode length (not too short/long)\",\n",
    "            \"‚úì Randomized starting conditions\",\n",
    "            \"‚úì Proper physics settings and constraints\",\n",
    "            \"‚úì Visual debugging aids available\"\n",
    "        ],\n",
    "        \"Agent Implementation\": [\n",
    "            \"‚úì Comprehensive observation space\",\n",
    "            \"‚úì Normalized observations when needed\",\n",
    "            \"‚úì Well-designed reward function\",\n",
    "            \"‚úì Proper action space dimensionality\",\n",
    "            \"‚úì Heuristic function for testing\"\n",
    "        ],\n",
    "        \"Training Configuration\": [\n",
    "            \"‚úì Behavior name matches between Unity and YAML\",\n",
    "            \"‚úì Appropriate hyperparameters for task complexity\",\n",
    "            \"‚úì Sufficient training steps allocated\",\n",
    "            \"‚úì Regular checkpoint saving enabled\",\n",
    "            \"‚úì TensorBoard logging configured\"\n",
    "        ],\n",
    "        \"Reward Engineering\": [\n",
    "            \"‚úì Positive rewards for desired behaviors\",\n",
    "            \"‚úì Negative rewards for undesired behaviors\",\n",
    "            \"‚úì Sparse rewards supplemented with dense rewards\",\n",
    "            \"‚úì Reward magnitude scaling appropriate\",\n",
    "            \"‚úì Terminal rewards clearly defined\"\n",
    "        ],\n",
    "        \"Training Process\": [\n",
    "            \"‚úì Start with simple scenarios\",\n",
    "            \"‚úì Monitor training progress regularly\",\n",
    "            \"‚úì Save models at key milestones\",\n",
    "            \"‚úì Test intermediate models\",\n",
    "            \"‚úì Document hyperparameter experiments\"\n",
    "        ],\n",
    "        \"Model Deployment\": [\n",
    "            \"‚úì ONNX model exported successfully\",\n",
    "            \"‚úì Model performance benchmarked\",\n",
    "            \"‚úì Inference speed acceptable for real-time use\",\n",
    "            \"‚úì Model behavior validated in target environment\",\n",
    "            \"‚úì Fallback mechanisms implemented\"\n",
    "        ],\n",
    "        \"Optimization\": [\n",
    "            \"‚úì Network architecture optimized for task\",\n",
    "            \"‚úì Training time vs performance balanced\",\n",
    "            \"‚úì Multiple training runs compared\",\n",
    "            \"‚úì Hyperparameter sensitivity analyzed\",\n",
    "            \"‚úì Curriculum learning considered for complex tasks\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"üìã Unity ML-Agents Best Practices Checklist\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    checklist_text = \"# Unity ML-Agents Best Practices Checklist\\n\\n\"\n",
    "    \n",
    "    for category, items in checklist.items():\n",
    "        print(f\"\\nüéØ {category}:\")\n",
    "        checklist_text += f\"## {category}\\n\\n\"\n",
    "        \n",
    "        for item in items:\n",
    "            print(f\"   {item}\")\n",
    "            checklist_text += f\"- [ ] {item[2:]}\\n\"  # Remove the ‚úì for markdown\n",
    "        \n",
    "        checklist_text += \"\\n\"\n",
    "    \n",
    "    # Save checklist\n",
    "    with open('Unity_ML_Project/Best_Practices_Checklist.md', 'w') as f:\n",
    "        f.write(checklist_text)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best practices checklist saved!\")\n",
    "    print(f\"üìÅ Saved to: Unity_ML_Project/Best_Practices_Checklist.md\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# Generate checklist\n",
    "best_practices = generate_best_practices_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Validation and Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and testing framework\n",
    "def create_validation_framework():\n",
    "    \"\"\"Create comprehensive validation framework for ML-Agents projects\"\"\"\n",
    "    \n",
    "    validation_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Unity ML-Agents Project Validation Script\"\"\"\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "class MLAgentsValidator:\n",
    "    def __init__(self, project_path):\n",
    "        self.project_path = project_path\n",
    "        self.results = []\n",
    "    \n",
    "    def validate_project_structure(self):\n",
    "        \"\"\"Validate project directory structure\"\"\"\n",
    "        required_dirs = [\n",
    "            'Training/configs',\n",
    "            'Training/results',\n",
    "            'Models',\n",
    "            'Scripts'\n",
    "        ]\n",
    "        \n",
    "        print(\"üîç Validating project structure...\")\n",
    "        \n",
    "        for dir_path in required_dirs:\n",
    "            full_path = os.path.join(self.project_path, dir_path)\n",
    "            if os.path.exists(full_path):\n",
    "                self.results.append(f\"‚úÖ Directory exists: {dir_path}\")\n",
    "            else:\n",
    "                self.results.append(f\"‚ùå Missing directory: {dir_path}\")\n",
    "    \n",
    "    def validate_config_files(self):\n",
    "        \"\"\"Validate YAML configuration files\"\"\"\n",
    "        config_dir = os.path.join(self.project_path, 'Training/configs')\n",
    "        \n",
    "        print(\"üîç Validating configuration files...\")\n",
    "        \n",
    "        if not os.path.exists(config_dir):\n",
    "            self.results.append(\"‚ùå Config directory not found\")\n",
    "            return\n",
    "        \n",
    "        yaml_files = [f for f in os.listdir(config_dir) if f.endswith('.yaml')]\n",
    "        \n",
    "        for yaml_file in yaml_files:\n",
    "            try:\n",
    "                with open(os.path.join(config_dir, yaml_file), 'r') as f:\n",
    "                    config = yaml.safe_load(f)\n",
    "                \n",
    "                # Validate structure\n",
    "                if 'behaviors' in config:\n",
    "                    self.results.append(f\"‚úÖ Valid config: {yaml_file}\")\n",
    "                else:\n",
    "                    self.results.append(f\"‚ùå Invalid config structure: {yaml_file}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.results.append(f\"‚ùå Config parse error in {yaml_file}: {str(e)}\")\n",
    "    \n",
    "    def validate_onnx_models(self):\n",
    "        \"\"\"Validate ONNX model files\"\"\"\n",
    "        models_dir = os.path.join(self.project_path, 'Models')\n",
    "        \n",
    "        print(\"üîç Validating ONNX models...\")\n",
    "        \n",
    "        if not os.path.exists(models_dir):\n",
    "            self.results.append(\"‚ùå Models directory not found\")\n",
    "            return\n",
    "        \n",
    "        onnx_files = [f for f in os.listdir(models_dir) if f.endswith('.onnx')]\n",
    "        \n",
    "        if not onnx_files:\n",
    "            self.results.append(\"‚ö†Ô∏è No ONNX models found\")\n",
    "            return\n",
    "        \n",
    "        for onnx_file in onnx_files:\n",
    "            try:\n",
    "                model_path = os.path.join(models_dir, onnx_file)\n",
    "                \n",
    "                # Load and validate\n",
    "                model = onnx.load(model_path)\n",
    "                onnx.checker.check_model(model)\n",
    "                \n",
    "                # Test inference\n",
    "                session = ort.InferenceSession(model_path)\n",
    "                \n",
    "                self.results.append(f\"‚úÖ Valid ONNX model: {onnx_file}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.results.append(f\"‚ùå ONNX validation failed for {onnx_file}: {str(e)}\")\n",
    "    \n",
    "    def run_validation(self):\n",
    "        \"\"\"Run complete validation suite\"\"\"\n",
    "        print(\"üöÄ Starting ML-Agents project validation...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.validate_project_structure()\n",
    "        self.validate_config_files()\n",
    "        self.validate_onnx_models()\n",
    "        \n",
    "        print(\"\\nüìä Validation Results:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for result in self.results:\n",
    "            print(result)\n",
    "        \n",
    "        # Summary\n",
    "        passed = len([r for r in self.results if r.startswith('‚úÖ')])\n",
    "        failed = len([r for r in self.results if r.startswith('‚ùå')])\n",
    "        warnings = len([r for r in self.results if r.startswith('‚ö†Ô∏è')])\n",
    "        \n",
    "        print(f\"\\nüìà Summary: {passed} passed, {failed} failed, {warnings} warnings\")\n",
    "        \n",
    "        return failed == 0\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    validator = MLAgentsValidator(\"Unity_ML_Project\")\n",
    "    success = validator.run_validation()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ Project validation passed!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Project validation failed. Please fix the issues above.\")\n",
    "    '''\n",
    "    \n",
    "    # Save validation script\n",
    "    with open('Unity_ML_Project/validate_project.py', 'w') as f:\n",
    "        f.write(validation_script)\n",
    "    \n",
    "    # Make it executable\n",
    "    os.chmod('Unity_ML_Project/validate_project.py', 0o755)\n",
    "    \n",
    "    print(\"‚úÖ Validation framework created!\")\n",
    "    print(\"üìÅ Saved to: Unity_ML_Project/validate_project.py\")\n",
    "    print(\"\\nüöÄ Usage: python Unity_ML_Project/validate_project.py\")\n",
    "\n",
    "# Create validation framework\n",
    "create_validation_framework()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Complete Workflow Summary\n",
    "\n",
    "### 10.1 End-to-End Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow pipeline\n",
    "def create_complete_workflow():\n",
    "    \"\"\"Create a complete end-to-end workflow script\"\"\"\n",
    "    \n",
    "    workflow_script = '''\n",
    "#!/bin/bash\n",
    "# Unity ML-Agents Complete Training Workflow\n",
    "# This script demonstrates the complete training pipeline\n",
    "\n",
    "echo \"üöÄ Unity ML-Agents Complete Training Workflow\"\n",
    "echo \"=============================================\"\n",
    "\n",
    "# Step 1: Validate project setup\n",
    "echo \"\\nüìã Step 1: Validating project setup...\"\n",
    "python validate_project.py\n",
    "\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"‚ùå Project validation failed. Please fix issues before proceeding.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Step 2: Start TensorBoard monitoring\n",
    "echo \"\\nüìä Step 2: Starting TensorBoard monitoring...\"\n",
    "tensorboard --logdir=Training/results --port=6006 &\n",
    "TENSORBOARD_PID=$!\n",
    "echo \"TensorBoard available at: http://localhost:6006\"\n",
    "\n",
    "# Step 3: Run basic training\n",
    "echo \"\\nüéØ Step 3: Starting basic training...\"\n",
    "mlagents-learn Training/configs/simple_agent_config.yaml --run-id=workflow_basic --train\n",
    "\n",
    "# Step 4: Export ONNX model\n",
    "echo \"\\nüì¶ Step 4: Model should be exported as ONNX automatically\"\n",
    "echo \"Check Training/results/workflow_basic/ for SimpleAgent.onnx\"\n",
    "\n",
    "# Step 5: Validate ONNX model\n",
    "echo \"\\nüîç Step 5: Validating ONNX model...\"\n",
    "python -c \"\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "try:\n",
    "    model = onnx.load('Training/results/workflow_basic/SimpleAgent.onnx')\n",
    "    onnx.checker.check_model(model)\n",
    "    session = ort.InferenceSession('Training/results/workflow_basic/SimpleAgent.onnx')\n",
    "    print('‚úÖ ONNX model validation passed!')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå ONNX validation failed: {e}')\n",
    "\"\n",
    "\n",
    "# Step 6: Copy model to deployment directory\n",
    "echo \"\\nüìÅ Step 6: Copying model to deployment directory...\"\n",
    "cp Training/results/workflow_basic/SimpleAgent.onnx Models/SimpleAgent_workflow.onnx\n",
    "echo \"‚úÖ Model copied to Models/SimpleAgent_workflow.onnx\"\n",
    "\n",
    "# Step 7: Benchmark model\n",
    "echo \"\\n‚ö° Step 7: Benchmarking model performance...\"\n",
    "python -c \"\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "session = ort.InferenceSession('Models/SimpleAgent_workflow.onnx')\n",
    "input_info = session.get_inputs()[0]\n",
    "test_input = np.random.randn(1, 10).astype(np.float32)\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    outputs = session.run(None, {input_info.name: test_input})\n",
    "end_time = time.time()\n",
    "\n",
    "avg_time_ms = ((end_time - start_time) / 1000) * 1000\n",
    "print(f'‚ö° Average inference time: {avg_time_ms:.3f}ms')\n",
    "\"\n",
    "\n",
    "# Step 8: Generate training report\n",
    "echo \"\\nüìä Step 8: Generating training report...\"\n",
    "python -c \"\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "report = f'''# Training Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Training Configuration\n",
    "- Config: simple_agent_config.yaml\n",
    "- Run ID: workflow_basic\n",
    "- Algorithm: PPO\n",
    "\n",
    "## Results\n",
    "- ONNX Model: ‚úÖ Generated successfully\n",
    "- Model Location: Models/SimpleAgent_workflow.onnx\n",
    "- Validation: ‚úÖ Passed\n",
    "\n",
    "## Next Steps\n",
    "1. Import ONNX model to Unity\n",
    "2. Configure Behavior Parameters\n",
    "3. Test agent performance\n",
    "4. Deploy to production environment\n",
    "\n",
    "## Files Generated\n",
    "- Training results: Training/results/workflow_basic/\n",
    "- ONNX model: Models/SimpleAgent_workflow.onnx\n",
    "- Logs: Available in TensorBoard\n",
    "'''\n",
    "\n",
    "with open('Training_Report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print('üìä Training report generated: Training_Report.md')\n",
    "\"\n",
    "\n",
    "# Cleanup\n",
    "echo \"\\nüßπ Cleaning up...\"\n",
    "kill $TENSORBOARD_PID 2>/dev/null\n",
    "\n",
    "echo \"\\nüéâ Workflow completed successfully!\"\n",
    "echo \"üìÅ Check the following files:\"\n",
    "echo \"   - Models/SimpleAgent_workflow.onnx (ONNX model)\"\n",
    "echo \"   - Training_Report.md (Training report)\"\n",
    "echo \"   - Training/results/workflow_basic/ (Training results)\"\n",
    "    '''\n",
    "    \n",
    "    # Save workflow script  \n",
    "    with open('Unity_ML_Project/complete_workflow.sh', 'w') as f:\n",
    "        f.write(workflow_script)\n",
    "    \n",
    "    # Make executable\n",
    "    os.chmod('Unity_ML_Project/complete_workflow.sh', 0o755)\n",
    "    \n",
    "    print(\"‚úÖ Complete workflow script created!\")\n",
    "    print(\"üìÅ Saved to: Unity_ML_Project/complete_workflow.sh\")\n",
    "    print(\"\\nüöÄ Usage: cd Unity_ML_Project && ./complete_workflow.sh\")\n",
    "\n",
    "# Create complete workflow\n",
    "create_complete_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Project Summary and Final Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final project summary\n",
    "def generate_project_summary():\n",
    "    \"\"\"Generate comprehensive project summary\"\"\"\n",
    "    \n",
    "    # List all created files\n",
    "    created_files = []\n",
    "    for root, dirs, files in os.walk('Unity_ML_Project'):\n",
    "        for file in files:\n",
    "            created_files.append(os.path.join(root, file))\n",
    "    \n",
    "    summary = f'''\n",
    "# Unity ML-Agents Complete Training Guide - Project Summary\n",
    "\n",
    "## üìÅ Generated Project Structure\n",
    "\n",
    "```\n",
    "Unity_ML_Project/\n",
    "‚îú‚îÄ‚îÄ Scripts/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ SimpleAgent.cs                    # C# Agent script for Unity\n",
    "‚îú‚îÄ‚îÄ Training/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ configs/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_agent_config.yaml      # Basic PPO configuration\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced_agent_config.yaml    # Advanced PPO with curiosity\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sac_agent_config.yaml         # SAC algorithm configuration\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ curriculum_config.yaml        # Curriculum learning setup\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiagent_config.yaml        # Multi-agent training\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sweep_config_*.yaml           # Hyperparameter sweep configs\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ results/                          # Training results directory\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ logs/                            # Training logs directory\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ run_hyperparameter_sweep.sh      # Automated hyperparameter tuning\n",
    "‚îú‚îÄ‚îÄ Models/                              # ONNX models directory\n",
    "‚îú‚îÄ‚îÄ complete_workflow.sh                 # Complete training pipeline\n",
    "‚îú‚îÄ‚îÄ validate_project.py                  # Project validation script\n",
    "‚îú‚îÄ‚îÄ Unity_ONNX_Integration_Guide.md      # Unity integration instructions\n",
    "‚îú‚îÄ‚îÄ Troubleshooting_Guide.md            # Common issues and solutions\n",
    "‚îî‚îÄ‚îÄ Best_Practices_Checklist.md         # Comprehensive best practices\n",
    "```\n",
    "\n",
    "## üöÄ Quick Start Guide\n",
    "\n",
    "### 1. Unity Setup\n",
    "1. Install Unity 2021.3 LTS or newer\n",
    "2. Create new Unity project\n",
    "3. Install ML-Agents package: `com.unity.ml-agents`\n",
    "4. Copy `Scripts/SimpleAgent.cs` to your Unity project\n",
    "5. Create training scene with Agent, Target, and Ground\n",
    "\n",
    "### 2. Python Environment\n",
    "```bash\n",
    "# Install required packages\n",
    "pip install mlagents torch matplotlib tensorboard onnx\n",
    "\n",
    "# Verify installation\n",
    "mlagents-learn --help\n",
    "```\n",
    "\n",
    "### 3. Training\n",
    "```bash\n",
    "# Basic training\n",
    "mlagents-learn Training/configs/simple_agent_config.yaml --run-id=my_first_run --train\n",
    "\n",
    "# Monitor with TensorBoard\n",
    "tensorboard --logdir=Training/results --port=6006\n",
    "```\n",
    "\n",
    "### 4. Model Deployment\n",
    "1. ONNX model automatically generated in `Training/results/my_first_run/`\n",
    "2. Import `.onnx` file to Unity\n",
    "3. Configure Behavior Parameters component\n",
    "4. Set Behavior Type to \"Inference Only\"\n",
    "\n",
    "## üéØ Key Features Covered\n",
    "\n",
    "### ‚úÖ Environment Setup\n",
    "- Unity and Python installation verification\n",
    "- ML-Agents package installation\n",
    "- Project structure creation\n",
    "\n",
    "### ‚úÖ Agent Development\n",
    "- Complete C# agent script with observations, actions, and rewards\n",
    "- Proper episode management and reset logic\n",
    "- Heuristic mode for manual testing\n",
    "\n",
    "### ‚úÖ Training Configuration\n",
    "- PPO and SAC algorithm configurations\n",
    "- Hyperparameter optimization setups\n",
    "- Curriculum learning examples\n",
    "- Multi-agent training configurations\n",
    "\n",
    "### ‚úÖ ONNX Integration\n",
    "- Automatic ONNX export during training\n",
    "- Model validation and testing utilities\n",
    "- Performance benchmarking tools\n",
    "- Unity integration guide\n",
    "\n",
    "### ‚úÖ Advanced Features\n",
    "- Curriculum learning for complex tasks\n",
    "- Multi-agent and self-play setups\n",
    "- Hyperparameter sweep automation\n",
    "- Training progress monitoring and analysis\n",
    "\n",
    "### ‚úÖ Quality Assurance\n",
    "- Comprehensive troubleshooting guide\n",
    "- Best practices checklist\n",
    "- Project validation framework\n",
    "- Complete workflow automation\n",
    "\n",
    "## üìä Training Commands Reference\n",
    "\n",
    "```bash\n",
    "# Basic training\n",
    "mlagents-learn Training/configs/simple_agent_config.yaml --run-id=basic_run --train\n",
    "\n",
    "# Advanced training with curiosity\n",
    "mlagents-learn Training/configs/advanced_agent_config.yaml --run-id=advanced_run --train\n",
    "\n",
    "# SAC algorithm\n",
    "mlagents-learn Training/configs/sac_agent_config.yaml --run-id=sac_run --train\n",
    "\n",
    "# Resume training\n",
    "mlagents-learn Training/configs/simple_agent_config.yaml --run-id=basic_run --resume --train\n",
    "\n",
    "# Training with build\n",
    "mlagents-learn Training/configs/simple_agent_config.yaml --run-id=build_run --env=path/to/build.exe --train\n",
    "```\n",
    "\n",
    "## üîß Utility Scripts\n",
    "\n",
    "- **validate_project.py**: Validates project setup and configuration\n",
    "- **complete_workflow.sh**: End-to-end training and deployment pipeline\n",
    "- **run_hyperparameter_sweep.sh**: Automated hyperparameter optimization\n",
    "\n",
    "## üìö Documentation Files\n",
    "\n",
    "- **Unity_ONNX_Integration_Guide.md**: Step-by-step Unity integration\n",
    "- **Troubleshooting_Guide.md**: Common issues and solutions\n",
    "- **Best_Practices_Checklist.md**: Comprehensive best practices\n",
    "\n",
    "## üéâ Success Metrics\n",
    "\n",
    "Your training is successful when:\n",
    "- ‚úÖ Agent consistently reaches target in training environment\n",
    "- ‚úÖ Cumulative reward shows upward trend in TensorBoard\n",
    "- ‚úÖ ONNX model exports without errors\n",
    "- ‚úÖ Model performs well in Unity inference mode\n",
    "- ‚úÖ Inference time is acceptable for real-time use (< 10ms)\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Customize the environment** for your specific use case\n",
    "2. **Experiment with different algorithms** (PPO vs SAC)\n",
    "3. **Optimize hyperparameters** using the sweep configurations\n",
    "4. **Implement curriculum learning** for complex tasks\n",
    "5. **Scale to multi-agent scenarios** when appropriate\n",
    "6. **Deploy to production** Unity environments\n",
    "\n",
    "---\n",
    "\n",
    "## üìû Support and Resources\n",
    "\n",
    "- **Official ML-Agents Documentation**: https://github.com/Unity-Technologies/ml-agents\n",
    "- **Unity ML-Agents Forum**: https://forum.unity.com/forums/ml-agents.453/\n",
    "- **TensorBoard Documentation**: https://www.tensorflow.org/tensorboard\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by Unity ML-Agents Complete Training Guide*\n",
    "*Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "    '''\n",
    "    \n",
    "    # Save summary\n",
    "    with open('Unity_ML_Project/PROJECT_SUMMARY.md', 'w') as f:\n",
    "        f.write(summary)\n",
    "    \n",
    "    print(\"‚úÖ Project summary generated!\")\n",
    "    print(\"üìÅ Saved to: Unity_ML_Project/PROJECT_SUMMARY.md\")\n",
    "    print(f\"\\nüìä Project Statistics:\")\n",
    "    print(f\"   Total files created: {len(created_files)}\")\n",
    "    print(f\"   Configuration files: {len([f for f in created_files if f.endswith('.yaml')])}\")\n",
    "    print(f\"   Documentation files: {len([f for f in created_files if f.endswith('.md')])}\")\n",
    "    print(f\"   Script files: {len([f for f in created_files if f.endswith('.py') or f.endswith('.sh') or f.endswith('.cs')])}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final summary\n",
    "project_summary = generate_project_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully completed the **Unity ML-Agents Complete Training Guide**!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ **Complete Development Environment Setup**\n",
    "- Python and Unity installation validation\n",
    "- ML-Agents package configuration\n",
    "- Project structure creation\n",
    "\n",
    "‚úÖ **Agent Development Mastery**\n",
    "- Full C# agent script with comprehensive features\n",
    "- Proper observation and action space design\n",
    "- Reward engineering best practices\n",
    "\n",
    "‚úÖ **Training Configuration Expertise**\n",
    "- Multiple algorithm configurations (PPO, SAC)\n",
    "- Advanced techniques (curriculum learning, multi-agent)\n",
    "- Hyperparameter optimization automation\n",
    "\n",
    "‚úÖ **ONNX Model Pipeline**\n",
    "- Automatic model export and validation\n",
    "- Performance benchmarking tools\n",
    "- Unity integration workflow\n",
    "\n",
    "‚úÖ **Production-Ready Workflow**\n",
    "- Complete automation scripts\n",
    "- Validation and testing frameworks\n",
    "- Comprehensive documentation\n",
    "\n",
    "### üöÄ You're Now Ready To:\n",
    "\n",
    "1. **Create sophisticated AI agents** for any Unity environment\n",
    "2. **Train models efficiently** with optimized configurations\n",
    "3. **Deploy ONNX models** seamlessly in Unity projects\n",
    "4. **Scale to complex scenarios** with advanced techniques\n",
    "5. **Troubleshoot and optimize** your ML-Agents projects\n",
    "\n",
    "### üìÅ All Files Available in: `Unity_ML_Project/`\n",
    "\n",
    "Start your ML-Agents journey by running:\n",
    "```bash\n",
    "cd Unity_ML_Project\n",
    "./complete_workflow.sh\n",
    "```\n",
    "\n",
    "**Happy Training! ü§ñüéÆ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}